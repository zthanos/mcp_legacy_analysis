[
    {
        "label": "cobol_context_prompt",
        "importPath": "templates.cobol_context_prompt",
        "description": "templates.cobol_context_prompt",
        "isExtraImport": true,
        "detail": "templates.cobol_context_prompt",
        "documentation": {}
    },
    {
        "label": "cobol_flow_extraction",
        "importPath": "templates.extract_cobol_template",
        "description": "templates.extract_cobol_template",
        "isExtraImport": true,
        "detail": "templates.extract_cobol_template",
        "documentation": {}
    },
    {
        "label": "clist_flow_extraction",
        "importPath": "templates.extract_clist_template",
        "description": "templates.extract_clist_template",
        "isExtraImport": true,
        "detail": "templates.extract_clist_template",
        "documentation": {}
    },
    {
        "label": "python_flow_extraction",
        "importPath": "templates.extract_python_template",
        "description": "templates.extract_python_template",
        "isExtraImport": true,
        "detail": "templates.extract_python_template",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "runpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "runpy",
        "description": "runpy",
        "detail": "runpy",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "path",
        "importPath": "os",
        "description": "os",
        "isExtraImport": true,
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "site",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "site",
        "description": "site",
        "detail": "site",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "socket",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "socket",
        "description": "socket",
        "detail": "socket",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "configparser",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "configparser",
        "description": "configparser",
        "detail": "configparser",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "Decimal",
        "importPath": "decimal",
        "description": "decimal",
        "isExtraImport": true,
        "detail": "decimal",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "upsert_repository",
        "importPath": "graph.graph_upsert",
        "description": "graph.graph_upsert",
        "isExtraImport": true,
        "detail": "graph.graph_upsert",
        "documentation": {}
    },
    {
        "label": "upsert_document",
        "importPath": "graph.graph_upsert",
        "description": "graph.graph_upsert",
        "isExtraImport": true,
        "detail": "graph.graph_upsert",
        "documentation": {}
    },
    {
        "label": "upsert_document",
        "importPath": "graph.graph_upsert",
        "description": "graph.graph_upsert",
        "isExtraImport": true,
        "detail": "graph.graph_upsert",
        "documentation": {}
    },
    {
        "label": "upsert_entry_point",
        "importPath": "graph.graph_upsert",
        "description": "graph.graph_upsert",
        "isExtraImport": true,
        "detail": "graph.graph_upsert",
        "documentation": {}
    },
    {
        "label": "upsert_internal_edge",
        "importPath": "graph.graph_upsert",
        "description": "graph.graph_upsert",
        "isExtraImport": true,
        "detail": "graph.graph_upsert",
        "documentation": {}
    },
    {
        "label": "upsert_external_edge",
        "importPath": "graph.graph_upsert",
        "description": "graph.graph_upsert",
        "isExtraImport": true,
        "detail": "graph.graph_upsert",
        "documentation": {}
    },
    {
        "label": "classify_file",
        "importPath": "analysis",
        "description": "analysis",
        "isExtraImport": true,
        "detail": "analysis",
        "documentation": {}
    },
    {
        "label": "get_file_content_full_path",
        "importPath": "analysis",
        "description": "analysis",
        "isExtraImport": true,
        "detail": "analysis",
        "documentation": {}
    },
    {
        "label": "get_file_content_full_path",
        "importPath": "analysis",
        "description": "analysis",
        "isExtraImport": true,
        "detail": "analysis",
        "documentation": {}
    },
    {
        "label": "get_file_content",
        "importPath": "analysis",
        "description": "analysis",
        "isExtraImport": true,
        "detail": "analysis",
        "documentation": {}
    },
    {
        "label": "get_file_full_path",
        "importPath": "graph_db",
        "description": "graph_db",
        "isExtraImport": true,
        "detail": "graph_db",
        "documentation": {}
    },
    {
        "label": "get_driver",
        "importPath": "graph_db",
        "description": "graph_db",
        "isExtraImport": true,
        "detail": "graph_db",
        "documentation": {}
    },
    {
        "label": "get_session",
        "importPath": "graph_db",
        "description": "graph_db",
        "isExtraImport": true,
        "detail": "graph_db",
        "documentation": {}
    },
    {
        "label": "get_session",
        "importPath": "graph_db",
        "description": "graph_db",
        "isExtraImport": true,
        "detail": "graph_db",
        "documentation": {}
    },
    {
        "label": "get_driver",
        "importPath": "graph_db",
        "description": "graph_db",
        "isExtraImport": true,
        "detail": "graph_db",
        "documentation": {}
    },
    {
        "label": "get_session",
        "importPath": "graph_db",
        "description": "graph_db",
        "isExtraImport": true,
        "detail": "graph_db",
        "documentation": {}
    },
    {
        "label": "insert_flow_graph",
        "importPath": "graph_db",
        "description": "graph_db",
        "isExtraImport": true,
        "detail": "graph_db",
        "documentation": {}
    },
    {
        "label": "get_session",
        "importPath": "graph_db",
        "description": "graph_db",
        "isExtraImport": true,
        "detail": "graph_db",
        "documentation": {}
    },
    {
        "label": "get_driver",
        "importPath": "graph_db",
        "description": "graph_db",
        "isExtraImport": true,
        "detail": "graph_db",
        "documentation": {}
    },
    {
        "label": "get_repository",
        "importPath": "graph_db",
        "description": "graph_db",
        "isExtraImport": true,
        "detail": "graph_db",
        "documentation": {}
    },
    {
        "label": "extract_edges_prompt",
        "importPath": "templates.extract_edges",
        "description": "templates.extract_edges",
        "isExtraImport": true,
        "detail": "templates.extract_edges",
        "documentation": {}
    },
    {
        "label": "extract_edges_prompt",
        "importPath": "templates.extract_edges",
        "description": "templates.extract_edges",
        "isExtraImport": true,
        "detail": "templates.extract_edges",
        "documentation": {}
    },
    {
        "label": "sample_helper",
        "importPath": "sampling",
        "description": "sampling",
        "isExtraImport": true,
        "detail": "sampling",
        "documentation": {}
    },
    {
        "label": "sample_helper",
        "importPath": "sampling",
        "description": "sampling",
        "isExtraImport": true,
        "detail": "sampling",
        "documentation": {}
    },
    {
        "label": "sample_helper",
        "importPath": "sampling",
        "description": "sampling",
        "isExtraImport": true,
        "detail": "sampling",
        "documentation": {}
    },
    {
        "label": "sample_helper",
        "importPath": "sampling",
        "description": "sampling",
        "isExtraImport": true,
        "detail": "sampling",
        "documentation": {}
    },
    {
        "label": "sample_helper",
        "importPath": "sampling",
        "description": "sampling",
        "isExtraImport": true,
        "detail": "sampling",
        "documentation": {}
    },
    {
        "label": "sample_helper",
        "importPath": "sampling",
        "description": "sampling",
        "isExtraImport": true,
        "detail": "sampling",
        "documentation": {}
    },
    {
        "label": "TextResource",
        "importPath": "fastmcp.resources",
        "description": "fastmcp.resources",
        "isExtraImport": true,
        "detail": "fastmcp.resources",
        "documentation": {}
    },
    {
        "label": "resource_manager",
        "importPath": "fastmcp.resources",
        "description": "fastmcp.resources",
        "isExtraImport": true,
        "detail": "fastmcp.resources",
        "documentation": {}
    },
    {
        "label": "TextResource",
        "importPath": "fastmcp.resources",
        "description": "fastmcp.resources",
        "isExtraImport": true,
        "detail": "fastmcp.resources",
        "documentation": {}
    },
    {
        "label": "TextResource",
        "importPath": "fastmcp.resources",
        "description": "fastmcp.resources",
        "isExtraImport": true,
        "detail": "fastmcp.resources",
        "documentation": {}
    },
    {
        "label": "resource_manager",
        "importPath": "fastmcp.resources",
        "description": "fastmcp.resources",
        "isExtraImport": true,
        "detail": "fastmcp.resources",
        "documentation": {}
    },
    {
        "label": "process_execution_flow",
        "importPath": "helpers.graph_flow_processor",
        "description": "helpers.graph_flow_processor",
        "isExtraImport": true,
        "detail": "helpers.graph_flow_processor",
        "documentation": {}
    },
    {
        "label": "upsert_technology_system",
        "importPath": "graph.graph_upsert_archimate",
        "description": "graph.graph_upsert_archimate",
        "isExtraImport": true,
        "detail": "graph.graph_upsert_archimate",
        "documentation": {}
    },
    {
        "label": "upsert_technology_artifact",
        "importPath": "graph.graph_upsert_archimate",
        "description": "graph.graph_upsert_archimate",
        "isExtraImport": true,
        "detail": "graph.graph_upsert_archimate",
        "documentation": {}
    },
    {
        "label": "upsert_technology_function",
        "importPath": "graph.graph_upsert_archimate",
        "description": "graph.graph_upsert_archimate",
        "isExtraImport": true,
        "detail": "graph.graph_upsert_archimate",
        "documentation": {}
    },
    {
        "label": "upsert_internal_flow",
        "importPath": "graph.graph_upsert_archimate",
        "description": "graph.graph_upsert_archimate",
        "isExtraImport": true,
        "detail": "graph.graph_upsert_archimate",
        "documentation": {}
    },
    {
        "label": "upsert_external_interaction",
        "importPath": "graph.graph_upsert_archimate",
        "description": "graph.graph_upsert_archimate",
        "isExtraImport": true,
        "detail": "graph.graph_upsert_archimate",
        "documentation": {}
    },
    {
        "label": "get_flow_extraction_prompt",
        "importPath": "templates.prompt_templates",
        "description": "templates.prompt_templates",
        "isExtraImport": true,
        "detail": "templates.prompt_templates",
        "documentation": {}
    },
    {
        "label": "get_flow_extraction_prompt",
        "importPath": "templates.prompt_templates",
        "description": "templates.prompt_templates",
        "isExtraImport": true,
        "detail": "templates.prompt_templates",
        "documentation": {}
    },
    {
        "label": "get_file_content_full_path",
        "importPath": "tools.document",
        "description": "tools.document",
        "isExtraImport": true,
        "detail": "tools.document",
        "documentation": {}
    },
    {
        "label": "retreive_document_info",
        "importPath": "tools.document",
        "description": "tools.document",
        "isExtraImport": true,
        "detail": "tools.document",
        "documentation": {}
    },
    {
        "label": "retreive_document_info",
        "importPath": "tools.document",
        "description": "tools.document",
        "isExtraImport": true,
        "detail": "tools.document",
        "documentation": {}
    },
    {
        "label": "classify_document",
        "importPath": "tools.document",
        "description": "tools.document",
        "isExtraImport": true,
        "detail": "tools.document",
        "documentation": {}
    },
    {
        "label": "get_file_content",
        "importPath": "tools.document",
        "description": "tools.document",
        "isExtraImport": true,
        "detail": "tools.document",
        "documentation": {}
    },
    {
        "label": "flow_extraction",
        "importPath": "templates.code_analyzer_prompt_generator",
        "description": "templates.code_analyzer_prompt_generator",
        "isExtraImport": true,
        "detail": "templates.code_analyzer_prompt_generator",
        "documentation": {}
    },
    {
        "label": "flow_extraction",
        "importPath": "templates.code_analyzer_prompt_generator",
        "description": "templates.code_analyzer_prompt_generator",
        "isExtraImport": true,
        "detail": "templates.code_analyzer_prompt_generator",
        "documentation": {}
    },
    {
        "label": "graph_to_json",
        "importPath": "helpers.response_helper",
        "description": "helpers.response_helper",
        "isExtraImport": true,
        "detail": "helpers.response_helper",
        "documentation": {}
    },
    {
        "label": "classify_file_template",
        "importPath": "templates.classify_file_template",
        "description": "templates.classify_file_template",
        "isExtraImport": true,
        "detail": "templates.classify_file_template",
        "documentation": {}
    },
    {
        "label": "classify_file_template",
        "importPath": "templates.classify_file_template",
        "description": "templates.classify_file_template",
        "isExtraImport": true,
        "detail": "templates.classify_file_template",
        "documentation": {}
    },
    {
        "label": "mock_response",
        "importPath": "tests.process_flow_response",
        "description": "tests.process_flow_response",
        "isExtraImport": true,
        "detail": "tests.process_flow_response",
        "documentation": {}
    },
    {
        "label": "complex_mock",
        "importPath": "tests.process_flow_response",
        "description": "tests.process_flow_response",
        "isExtraImport": true,
        "detail": "tests.process_flow_response",
        "documentation": {}
    },
    {
        "label": "convert_llm_steps_to_flow",
        "importPath": "helpers.llm_to_flow_graph",
        "description": "helpers.llm_to_flow_graph",
        "isExtraImport": true,
        "detail": "helpers.llm_to_flow_graph",
        "documentation": {}
    },
    {
        "label": "convert_llm_steps_to_flow",
        "importPath": "helpers.llm_to_flow_graph",
        "description": "helpers.llm_to_flow_graph",
        "isExtraImport": true,
        "detail": "helpers.llm_to_flow_graph",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "GraphDatabase",
        "importPath": "neo4j",
        "description": "neo4j",
        "isExtraImport": true,
        "detail": "neo4j",
        "documentation": {}
    },
    {
        "label": "GraphDatabase",
        "importPath": "neo4j",
        "description": "neo4j",
        "isExtraImport": true,
        "detail": "neo4j",
        "documentation": {}
    },
    {
        "label": "sqlite3",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sqlite3",
        "description": "sqlite3",
        "detail": "sqlite3",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "mcp",
        "importPath": "legacy_analysis_server",
        "description": "legacy_analysis_server",
        "isExtraImport": true,
        "detail": "legacy_analysis_server",
        "documentation": {}
    },
    {
        "label": "prepare_bms_analysis_prompt",
        "importPath": "templates.analyze_cobol_map",
        "description": "templates.analyze_cobol_map",
        "isExtraImport": true,
        "detail": "templates.analyze_cobol_map",
        "documentation": {}
    },
    {
        "label": "FastMCP",
        "importPath": "fastmcp",
        "description": "fastmcp",
        "isExtraImport": true,
        "detail": "fastmcp",
        "documentation": {}
    },
    {
        "label": "Context",
        "importPath": "fastmcp",
        "description": "fastmcp",
        "isExtraImport": true,
        "detail": "fastmcp",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "fastmcp",
        "description": "fastmcp",
        "isExtraImport": true,
        "detail": "fastmcp",
        "documentation": {}
    },
    {
        "label": "FastMCP",
        "importPath": "fastmcp",
        "description": "fastmcp",
        "isExtraImport": true,
        "detail": "fastmcp",
        "documentation": {}
    },
    {
        "label": "execute_fetch_repository",
        "importPath": "tools.fetch_repository",
        "description": "tools.fetch_repository",
        "isExtraImport": true,
        "detail": "tools.fetch_repository",
        "documentation": {}
    },
    {
        "label": "execute_classify_repository",
        "importPath": "tools.classify_repository",
        "description": "tools.classify_repository",
        "isExtraImport": true,
        "detail": "tools.classify_repository",
        "documentation": {}
    },
    {
        "label": "execute_expose_workspace",
        "importPath": "tools.expose_workspace",
        "description": "tools.expose_workspace",
        "isExtraImport": true,
        "detail": "tools.expose_workspace",
        "documentation": {}
    },
    {
        "label": "extract_document_flow",
        "importPath": "tools.extract_document_flow",
        "description": "tools.extract_document_flow",
        "isExtraImport": true,
        "detail": "tools.extract_document_flow",
        "documentation": {}
    },
    {
        "label": "extract_language_specific_flow",
        "importPath": "tools.extract_document_flow",
        "description": "tools.extract_document_flow",
        "isExtraImport": true,
        "detail": "tools.extract_document_flow",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "ollama",
        "description": "ollama",
        "isExtraImport": true,
        "detail": "ollama",
        "documentation": {}
    },
    {
        "label": "SamplingMessage",
        "importPath": "fastmcp.client.sampling",
        "description": "fastmcp.client.sampling",
        "isExtraImport": true,
        "detail": "fastmcp.client.sampling",
        "documentation": {}
    },
    {
        "label": "SamplingParams",
        "importPath": "fastmcp.client.sampling",
        "description": "fastmcp.client.sampling",
        "isExtraImport": true,
        "detail": "fastmcp.client.sampling",
        "documentation": {}
    },
    {
        "label": "RequestContext",
        "importPath": "mcp.shared.context",
        "description": "mcp.shared.context",
        "isExtraImport": true,
        "detail": "mcp.shared.context",
        "documentation": {}
    },
    {
        "label": "tabulate",
        "importPath": "tabulate",
        "description": "tabulate",
        "isExtraImport": true,
        "detail": "tabulate",
        "documentation": {}
    },
    {
        "label": "cobol_flow_extraction",
        "kind": 2,
        "importPath": "templates.extract_cobol_template",
        "description": "templates.extract_cobol_template",
        "peekOfCode": "def cobol_flow_extraction(filename: str, source_code: str) -> tuple[str, str]:\n    system_prompt = cobol_context_prompt\n    llm_message = f\"\"\"\nYou are an expert COBOL programmer and a seasoned static analysis tool.\nYour task is to analyze the provided COBOL code and extract its execution flow, starting from its primary entry point (typically the first executable statement after PROCEDURE DIVISION, or an explicit ENTRY point).\nRepresent the flow as a directed graph in JSON format, where:\n- Each node is a COBOL paragraph, section, or program/subprogram name.\n- Each edge represents a control flow transfer (e.g., PERFORM, CALL, GO TO, implicit fall-through).\n- For each node, include:\n  - \"type\" (e.g., \"paragraph\", \"section\", \"program\")",
        "detail": "templates.extract_cobol_template",
        "documentation": {}
    },
    {
        "label": "get_flow_extraction_prompt",
        "kind": 2,
        "importPath": "templates.prompt_templates",
        "description": "templates.prompt_templates",
        "peekOfCode": "def get_flow_extraction_prompt(filename: str, classification: str, source_code: str) -> tuple[str, str]:\n    classification = classification.lower()\n    if classification == \"cobol\":\n        return cobol_flow_extraction(filename=filename, source_code=source_code)\n    elif classification == \"clist\":\n        return clist_flow_extraction(filename=filename, source_code=source_code)    \n    elif classification == \"python\":\n        return python_flow_extraction(filename=filename, source_code=source_code)\n    else:\n        raise ValueError(f\"Unknown classification: {classification}\")",
        "detail": "templates.prompt_templates",
        "documentation": {}
    },
    {
        "label": "python_flow_extraction",
        "kind": 2,
        "importPath": "templates.extract_python_template",
        "description": "templates.extract_python_template",
        "peekOfCode": "def python_flow_extraction(source_code: str, filename: str, program_id: str=\"program_id\") -> tuple[str, str]:\n    system_prompt = (\n        \"You are an expert in Python programming, static code analysis, and legacy system mapping.\\n\"\n        \"Your task is to extract the execution flow of the provided Python source code and produce a valid JSON representation.\\n\"\n        \"This analysis is for system-wide integration mapping and impact assessment, especially detecting external dependencies.\\n\"\n    )\n    llm_messages = f\"\"\"\nAnalyze the following Python source code strictly based on the provided content (static analysis only, no assumptions).\nYour output must be a valid JSON matching this structure:\n{{",
        "detail": "templates.extract_python_template",
        "documentation": {}
    },
    {
        "label": "clist_flow_extraction",
        "kind": 2,
        "importPath": "templates.extract_clist_template",
        "description": "templates.extract_clist_template",
        "peekOfCode": "def clist_flow_extraction(filename: str, source_code: str) -> tuple[str, str]:\n    system_prompt = (\n        \"You are an expert in analyzing TSO/ISPF CLIST scripts for execution flow extraction.\\n\"\n        \"Your task is to extract the control flow of the provided CLIST script and represent it as a directed graph.\\n\"\n        \"Focus on control structures (IF, DO, SELECT), label jumps (GOTO), procedure calls, and system command execution.\\n\"\n        \"Ensure output follows exactly the JSON structure below, matching the COBOL model for system-wide consistency.\"\n    )\n    llm_message = f\"\"\"\nAnalyze the following CLIST script and return a valid JSON matching this structure:\n{{",
        "detail": "templates.extract_clist_template",
        "documentation": {}
    },
    {
        "label": "classify_file_template",
        "kind": 2,
        "importPath": "templates.classify_file_template",
        "description": "templates.classify_file_template",
        "peekOfCode": "def classify_file_template(filename, content: str, repository: str) -> tuple[str, str]:\n    system_prompt = \"\"\"You are an expert in programming languages and file formats, including legacy \nmainframe technologies such as COBOL, CLIST, JCL, BMS maps, and others. Pay special attention to \nCLIST files which typically start with PROC statements and use TSO commands like ALLOC, FREE, and CONTROL.\"\"\"\n    llm_message = f\"\"\"\n### You must return ONLY a JSON object with the following format:\n{{\n  \"filename\": \"example.clist\",\n  \"repository\": \"SAMPLE-REPO\",\n  \"classification\": \"CLIST Script\",",
        "detail": "templates.classify_file_template",
        "documentation": {}
    },
    {
        "label": "cobol_context_prompt",
        "kind": 5,
        "importPath": "templates.cobol_context_prompt",
        "description": "templates.cobol_context_prompt",
        "peekOfCode": "cobol_context_prompt = \"\"\"\nCOBOL (Common Business-Oriented Language) is a procedural, structured language widely used in legacy enterprise systems such as banking, insurance, and government infrastructure.\nA COBOL program consists of four main DIVISIONS:\n1. **IDENTIFICATION DIVISION**: Contains metadata such as the program name.\n2. **ENVIRONMENT DIVISION**: Describes input/output devices and runtime context.\n3. **DATA DIVISION**: Declares variables, files, records, constants, and memory layouts.\n4. **PROCEDURE DIVISION**: Contains the actual business logic and control flow.\n---\n### ðŸ’¡ Key Syntax and Concepts:\n#### ðŸ§± Program Structure:",
        "detail": "templates.cobol_context_prompt",
        "documentation": {}
    },
    {
        "label": "extract_useful_bms_lines",
        "kind": 2,
        "importPath": "templates.analyze_cobol_map",
        "description": "templates.analyze_cobol_map",
        "peekOfCode": "def extract_useful_bms_lines(content: str) -> str:\n    \"\"\"\n    ÎšÏÎ±Ï„Î¬ Î¼ÏŒÎ½Î¿:\n    - MAP ÏŒÎ½Î¿Î¼Î± Î±Ï€ÏŒ DFHMDI\n    - ÎŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ labeled DFHMDF Î³ÏÎ±Î¼Î¼Î­Ï‚\n    \"\"\"\n    lines = content.splitlines()\n    useful_lines = []\n    inside_map = False\n    for line in lines:",
        "detail": "templates.analyze_cobol_map",
        "documentation": {}
    },
    {
        "label": "prepare_bms_analysis_prompt",
        "kind": 2,
        "importPath": "templates.analyze_cobol_map",
        "description": "templates.analyze_cobol_map",
        "peekOfCode": "def prepare_bms_analysis_prompt(clean_content: str) -> tuple[str, str]:\n    system_prompt = \"You are an expert in COBOL, HLASM, and CICS BMS MAP screen parsing. Output only valid JSON, no explanations.\"\n    llm_message = f\"\"\"\nI will provide a filtered BMS copybook definition written in HLASM syntax.\nYour task:\n- Locate all labeled DFHMDF fields\n- Output a JSON array with:\n    - `name`: Field name as defined before DFHMDF\n    - `type`: 'X' for alphanumeric, '9' for numeric\n    - `size`: Length from LENGTH attribute",
        "detail": "templates.analyze_cobol_map",
        "documentation": {}
    },
    {
        "label": "extract_edges_prompt",
        "kind": 2,
        "importPath": "templates.extract_edges",
        "description": "templates.extract_edges",
        "peekOfCode": "def extract_edges_prompt(content: str) -> tuple[str, str]:\n    system_prompt = \"You are an expert COBOL programmer and a seasoned static analysis tool.\"\n    llm_message = f\"\"\"\nYou will analyze the following source code and identify all points of interaction with external or internal components. This includes:\n- File operations such as `READ`, `WRITE`, `OPEN`, `CLOSE`, `SELECT`, `ASSIGN`\n- Database access via `EXEC SQL`, embedded SQL statements\n- Calls to other programs or modules using `CALL`\n- Service or transaction instructions like `EXEC TRU`, `EXEC CICS`, `SEND`, `RECEIVE`\nFor each interaction, return a JSON object containing:\n- `edge_type`: one of `file`, `database`, `service`, `program`",
        "detail": "templates.extract_edges",
        "documentation": {}
    },
    {
        "label": "generate_common_json_template",
        "kind": 2,
        "importPath": "templates.code_analyzer_prompt_generator",
        "description": "templates.code_analyzer_prompt_generator",
        "peekOfCode": "def generate_common_json_template(program_id: str, filename: str, repository_name: str, language: str, entry_points_hint: str, source_code: str) -> str:\n    return f\"\"\"\nProduce valid JSON in this format:\n{{\n  \"program_id\": \"{program_id}\",\n  \"filename\": \"{filename}\",\n  \"repository_name\": \"{repository_name}\",\n  \"language\": \"{language}\",\n  \"main_entry_points\": [{entry_points_hint}],\n  \"flow_graph\": [",
        "detail": "templates.code_analyzer_prompt_generator",
        "documentation": {}
    },
    {
        "label": "languages_from_json",
        "kind": 2,
        "importPath": "templates.code_analyzer_prompt_generator",
        "description": "templates.code_analyzer_prompt_generator",
        "peekOfCode": "def languages_from_json(filepath: str) -> dict:\n    import json\n    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\ndef generate_common_system_prompt(language: str) -> str:\n    return f\"You are a {language} static code analysis expert for legacy systems. Extract execution flow, external interactions, inheritance, and critical paths.\"\ndef flow_extraction(source_code: str, filename: str, repository_name: str, program_id: str, language: str) -> tuple[str, str]:\n    supported = languages_from_json(\"languages_config.json\")\n    if language not in supported:\n        raise ValueError(f\"Unsupported language: {language}\")",
        "detail": "templates.code_analyzer_prompt_generator",
        "documentation": {}
    },
    {
        "label": "generate_common_system_prompt",
        "kind": 2,
        "importPath": "templates.code_analyzer_prompt_generator",
        "description": "templates.code_analyzer_prompt_generator",
        "peekOfCode": "def generate_common_system_prompt(language: str) -> str:\n    return f\"You are a {language} static code analysis expert for legacy systems. Extract execution flow, external interactions, inheritance, and critical paths.\"\ndef flow_extraction(source_code: str, filename: str, repository_name: str, program_id: str, language: str) -> tuple[str, str]:\n    supported = languages_from_json(\"languages_config.json\")\n    if language not in supported:\n        raise ValueError(f\"Unsupported language: {language}\")\n    if language not in supported:\n        raise ValueError(f\"Unsupported language: {language}\")\n    system_prompt = generate_common_system_prompt(language)\n    print(f\"system_prompt: {system_prompt}\")",
        "detail": "templates.code_analyzer_prompt_generator",
        "documentation": {}
    },
    {
        "label": "flow_extraction",
        "kind": 2,
        "importPath": "templates.code_analyzer_prompt_generator",
        "description": "templates.code_analyzer_prompt_generator",
        "peekOfCode": "def flow_extraction(source_code: str, filename: str, repository_name: str, program_id: str, language: str) -> tuple[str, str]:\n    supported = languages_from_json(\"languages_config.json\")\n    if language not in supported:\n        raise ValueError(f\"Unsupported language: {language}\")\n    if language not in supported:\n        raise ValueError(f\"Unsupported language: {language}\")\n    system_prompt = generate_common_system_prompt(language)\n    print(f\"system_prompt: {system_prompt}\")\n    entry_points_hint = supported[language][\"entry_points_hint\"]\n    llm_message = generate_common_json_template(program_id, filename, repository_name, language, entry_points_hint, source_code)",
        "detail": "templates.code_analyzer_prompt_generator",
        "documentation": {}
    },
    {
        "label": "bin_dir",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "bin_dir = os.path.dirname(abs_file)\nbase = bin_dir[: -len(\"Scripts\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"mcp-legacy-analysis\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "base",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "base = bin_dir[: -len(\"Scripts\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"mcp-legacy-analysis\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PATH\"]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"mcp-legacy-analysis\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV\"]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"mcp-legacy-analysis\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV_PROMPT\"]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV_PROMPT\"] = \"mcp-legacy-analysis\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "prev_length",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "prev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.path[:]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "sys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.real_prefix",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "sys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.prefix",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "sys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "generate_fake_records",
        "kind": 2,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "def generate_fake_records(number_of_records=100):\n    ''' Generates fake records JCL '''\n    fake_labels = ['CIBC', 'DOGE Bank LLC', 'SUCH FUNDS', 'WOW MONEY','Fake','Banco do Brazil','Kraken','MTGOX']\n    logger.debug(\"Generating {} fake records.\".format(number_of_records))\n    record = \"{key:010d} {address:<034} {label:<10.10} {amount:+018.8f}\"\n    records = []\n    records.append(record.format(key=1,address=0,label=\"Available\", amount=+87654321.12345678))\n    records.append(record.format(key=2,address=0,label=\"Pending\", amount=-123456.654321))\n    for i in range(1,int(number_of_records)):\n        records.append(record.format(key=random.randint(1000000000,int(time.time())),address=\"nYLEKeZtqNSCAhMNKTFpFgZcnvf1DbFiSu\",label=fake_labels[random.randint(0,7)], amount=random.uniform(-10000000,10000000)))",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "get_records",
        "kind": 2,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "def get_records(host='localhost', rpcUser=None, rpcPass=None, rpcPort=22555):\n    ''' Gets DOGECOIN records from dogecoin RPC server '''\n    try:\n        with open(path.join(path.expanduser(\"~\"), '.dogecoin', 'dogecoin.conf'), mode='r') as f:\n            config_string = '[dogecoin]\\n' + f.read()\n    except:\n        config_string = None\n    config = configparser.ConfigParser()\n    config.read_string(config_string)\n    if not rpcUser and 'rpcuser' in config['dogecoin']:",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "def test(user='DOGE', password='DOGECOIN',target='localhost', port=3505):\n    ''' send IEFBR14 job to hercules sockdev '''\n    logger.debug(\"Sending IEFBR14 to {}:{}\".format(target,port))\n    send_jcl(hostname=target, port=port, jcl=IEFBR14.format(user=user,password=password))\ndef test_print(user='DOGE', password='DOGECOIN',target='localhost', port=3505):\n    ''' send IEFBR14 job to hercules sockdev '''\n    print(IEFBR14.format(user=user,password=password))\ndef send_jcl(hostname='localhost',port=3505, jcl=\"\", print_jcl=False):\n    logger.debug(\"Sending VSAM update JCL to tk4- reader using {}:{}\".format(hostname,port))\n    if print_jcl:",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "test_print",
        "kind": 2,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "def test_print(user='DOGE', password='DOGECOIN',target='localhost', port=3505):\n    ''' send IEFBR14 job to hercules sockdev '''\n    print(IEFBR14.format(user=user,password=password))\ndef send_jcl(hostname='localhost',port=3505, jcl=\"\", print_jcl=False):\n    logger.debug(\"Sending VSAM update JCL to tk4- reader using {}:{}\".format(hostname,port))\n    if print_jcl:\n        print(\"PRINTING JCL:\\n{}\\n{}\\n{}\\n\".format('-'*80,jcl, '-'*80))\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.connect((hostname,port))\n    s.sendall(jcl.encode())",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "send_jcl",
        "kind": 2,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "def send_jcl(hostname='localhost',port=3505, jcl=\"\", print_jcl=False):\n    logger.debug(\"Sending VSAM update JCL to tk4- reader using {}:{}\".format(hostname,port))\n    if print_jcl:\n        print(\"PRINTING JCL:\\n{}\\n{}\\n{}\\n\".format('-'*80,jcl, '-'*80))\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.connect((hostname,port))\n    s.sendall(jcl.encode())\n    s.close()\ndef generate_IDCAMS_JCL(user='herc01',password='cul8tr',vsam_file='DOGE.VSAM',records='', volume='pub012', reverse=True):\n    if len(records) > 7648:",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "generate_IDCAMS_JCL",
        "kind": 2,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "def generate_IDCAMS_JCL(user='herc01',password='cul8tr',vsam_file='DOGE.VSAM',records='', volume='pub012', reverse=True):\n    if len(records) > 7648:\n        if reverse:\n            logger.debug(\"Records exceeds maximum records length of 7648. Getting last 7648 records. To get first 7648 records use --start-records-at-one\")\n            record0000000001 = records[0]\n            record0000000002 = records[1]\n            records = records[-7648:]\n            records[0] = record0000000001\n            records[1] = record0000000002\n        else:",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "new_records",
        "kind": 2,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "def new_records(old_records, new_records):\n    if old_records == new_records:\n        logger.debug(\"no new records, update not required, force update with --force\".format(running_folder,tmp_file))\n        return False\n    else:\n        logger.debug(\"new records in wallet, sending update\")\n        return True\ndef get_commands(timeout=2, hostname='localhost', port=3506):\n# From https://www.binarytides.com/receive-full-data-with-the-recv-socket-function-in-python/\n    logger.debug('Connecting to tk4- printer {}:{} to get transactions.'.format(hostname,port))",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "get_commands",
        "kind": 2,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "def get_commands(timeout=2, hostname='localhost', port=3506):\n# From https://www.binarytides.com/receive-full-data-with-the-recv-socket-function-in-python/\n    logger.debug('Connecting to tk4- printer {}:{} to get transactions.'.format(hostname,port))\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.connect((hostname,port))\n    s.setblocking(0)\n    total_data=[]\n    data=''\n    begin=time.time()\n    while 1:",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "send_doge",
        "kind": 2,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "def send_doge(address, amount=0, host='localhost', rpcUser=None, rpcPass=None, rpcPort=22555):\n    ''' Sends amount of dogecoin to address '''\n    logger.debug('Connecting to {}:{} to send {} to {}'.format(host,rpcPort, amount, address))\n    try:\n        with open(path.join(path.expanduser(\"~\"), '.dogecoin', 'dogecoin.conf'), mode='r') as f:\n            config_string = '[dogecoin]\\n' + f.read()\n    except:\n        config_string = None\n    config = configparser.ConfigParser()\n    config.read_string(config_string)",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "tmp_file",
        "kind": 5,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "tmp_file = \"doge.tmp\"\nrunning_folder = os.path.dirname(os.path.abspath(__file__))\nIEFBR14 = '''//DOGEBR14 JOB CLASS=C,MSGCLASS=Z,MSGLEVEL=(1,1),\n//*        NOTIFY={user},\n//        USER={user},PASSWORD={password}\n//DOGELOL EXEC PGM=IEFBR14''' \nIDCAMS = '''//DOGEVSM JOB (BAL),\n//             'DOGEBANK VSAM',\n//             CLASS=A,\n//             MSGCLASS=Z,",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "running_folder",
        "kind": 5,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "running_folder = os.path.dirname(os.path.abspath(__file__))\nIEFBR14 = '''//DOGEBR14 JOB CLASS=C,MSGCLASS=Z,MSGLEVEL=(1,1),\n//*        NOTIFY={user},\n//        USER={user},PASSWORD={password}\n//DOGELOL EXEC PGM=IEFBR14''' \nIDCAMS = '''//DOGEVSM JOB (BAL),\n//             'DOGEBANK VSAM',\n//             CLASS=A,\n//             MSGCLASS=Z,\n//             TIME=1440,",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "IEFBR14",
        "kind": 5,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "IEFBR14 = '''//DOGEBR14 JOB CLASS=C,MSGCLASS=Z,MSGLEVEL=(1,1),\n//*        NOTIFY={user},\n//        USER={user},PASSWORD={password}\n//DOGELOL EXEC PGM=IEFBR14''' \nIDCAMS = '''//DOGEVSM JOB (BAL),\n//             'DOGEBANK VSAM',\n//             CLASS=A,\n//             MSGCLASS=Z,\n//             TIME=1440,\n//             MSGLEVEL=(1,1),",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "IDCAMS",
        "kind": 5,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "IDCAMS = '''//DOGEVSM JOB (BAL),\n//             'DOGEBANK VSAM',\n//             CLASS=A,\n//             MSGCLASS=Z,\n//             TIME=1440,\n//             MSGLEVEL=(1,1),\n//*             NOTIFY={user},\n//             USER={user},PASSWORD={password}\n//DOGECAMS EXEC PGM=IDCAMS\n//SYSPRINT DD   SYSOUT=*",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "desc",
        "kind": 5,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "desc = '''DOGEdcams data generator for DOGE Bank. Used to send and receive funds between KICKS on TK4- and DogeCICS.'''\narg_parser = argparse.ArgumentParser(description=desc, \n                    usage='%(prog)s [options]', \n                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\narg_parser.add_argument('-d', '--debug', help=\"Print lots of debugging statements\", action=\"store_const\", dest=\"loglevel\", const=logging.DEBUG, default=logging.WARNING)\narg_parser.add_argument('-t', '--test', help=\"Test sending JCL to TK4-\", action=\"store_true\")\narg_parser.add_argument('-p', '--print', help=\"Print JCL being sent to TK4\", action=\"store_true\")\narg_parser.add_argument('-f', '--force', help=\"Force VSAM update even if no changes to wallet\", action=\"store_true\")\narg_parser.add_argument('--fake', help=\"Generate fake records\", default=None)\narg_parser.add_argument('--username', help=\"TK4- username for JCL\", default='herc01')",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "arg_parser",
        "kind": 5,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "arg_parser = argparse.ArgumentParser(description=desc, \n                    usage='%(prog)s [options]', \n                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\narg_parser.add_argument('-d', '--debug', help=\"Print lots of debugging statements\", action=\"store_const\", dest=\"loglevel\", const=logging.DEBUG, default=logging.WARNING)\narg_parser.add_argument('-t', '--test', help=\"Test sending JCL to TK4-\", action=\"store_true\")\narg_parser.add_argument('-p', '--print', help=\"Print JCL being sent to TK4\", action=\"store_true\")\narg_parser.add_argument('-f', '--force', help=\"Force VSAM update even if no changes to wallet\", action=\"store_true\")\narg_parser.add_argument('--fake', help=\"Generate fake records\", default=None)\narg_parser.add_argument('--username', help=\"TK4- username for JCL\", default='herc01')\narg_parser.add_argument('--password', help=\"TK4- password for JCL\", default='cul8tr')",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "args = arg_parser.parse_args()\t\n# Create the Logger\nlogger = logging.getLogger(__name__)\nlogger.setLevel(args.loglevel)\nlogger_formatter = logging.Formatter('%(levelname)-8s :: %(funcName)-22s :: %(message)s')\nch = logging.StreamHandler()\nch.setFormatter(logger_formatter)\nch.setLevel(args.loglevel)\nlogger.addHandler(ch)\n# Print debug information",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "logger = logging.getLogger(__name__)\nlogger.setLevel(args.loglevel)\nlogger_formatter = logging.Formatter('%(levelname)-8s :: %(funcName)-22s :: %(message)s')\nch = logging.StreamHandler()\nch.setFormatter(logger_formatter)\nch.setLevel(args.loglevel)\nlogger.addHandler(ch)\n# Print debug information\nlogger.debug(\"Using the following script options - Debug: True, Test: {}, Print: {}, Force: {}\".format(args.test, args.print, args.force))\nlogger.debug(\"Using the following TK4- options - Hostname: {}, Username: {}, Password: {}, VSAM File: {}, Volume: {}, Reader Port: {}, Printer Port: {}\".format(",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "logger_formatter",
        "kind": 5,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "logger_formatter = logging.Formatter('%(levelname)-8s :: %(funcName)-22s :: %(message)s')\nch = logging.StreamHandler()\nch.setFormatter(logger_formatter)\nch.setLevel(args.loglevel)\nlogger.addHandler(ch)\n# Print debug information\nlogger.debug(\"Using the following script options - Debug: True, Test: {}, Print: {}, Force: {}\".format(args.test, args.print, args.force))\nlogger.debug(\"Using the following TK4- options - Hostname: {}, Username: {}, Password: {}, VSAM File: {}, Volume: {}, Reader Port: {}, Printer Port: {}\".format(\n            args.hostname, args.username, \"*\"*len(args.password), args.vsam_file, args.volume, args.rdrport, args.prtport))\nlogger.debug(\"Using the following Dogecoin options - RPC Host: {}, RPC User: {}, RPC Pass: {} RPC Port: {}\".format(",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "ch",
        "kind": 5,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "ch = logging.StreamHandler()\nch.setFormatter(logger_formatter)\nch.setLevel(args.loglevel)\nlogger.addHandler(ch)\n# Print debug information\nlogger.debug(\"Using the following script options - Debug: True, Test: {}, Print: {}, Force: {}\".format(args.test, args.print, args.force))\nlogger.debug(\"Using the following TK4- options - Hostname: {}, Username: {}, Password: {}, VSAM File: {}, Volume: {}, Reader Port: {}, Printer Port: {}\".format(\n            args.hostname, args.username, \"*\"*len(args.password), args.vsam_file, args.volume, args.rdrport, args.prtport))\nlogger.debug(\"Using the following Dogecoin options - RPC Host: {}, RPC User: {}, RPC Pass: {} RPC Port: {}\".format(\n            args.rpchost, args.rpcuser, \"*\"*len(args.rpchost), args.rpcport))",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "doge_vsam_jcl",
        "kind": 5,
        "importPath": "workspace.DOGECICS.PYTHON.dogedcams",
        "description": "workspace.DOGECICS.PYTHON.dogedcams",
        "peekOfCode": "doge_vsam_jcl = generate_IDCAMS_JCL(user=args.username,password=args.password,vsam_file=args.vsam_file,volume=args.volume,records=vsam_records, reverse=args.start_records_at_one)\nif not os.path.isfile(\"{}/{}\".format(running_folder,tmp_file)) or args.force:\n    # If the tmp file doesn't exist or we need to force an update for some reason\n    if not os.path.isfile(\"{}/{}\".format(running_folder,tmp_file)):\n        logger.debug(\"temp file {}/{} does not exist, creating\".format(running_folder,tmp_file))\n    else:\n        logger.debug(\"forced update\")\n    if not args.test:\n        send_jcl(hostname=args.hostname,port=args.rdrport, jcl=doge_vsam_jcl, print_jcl=args.print)\n        logger.debug(\"creating: {}/{}\".format(running_folder,tmp_file) )",
        "detail": "workspace.DOGECICS.PYTHON.dogedcams",
        "documentation": {}
    },
    {
        "label": "extract_alias_from_url",
        "kind": 2,
        "importPath": "tools.fetch_repository",
        "description": "tools.fetch_repository",
        "peekOfCode": "def extract_alias_from_url(repo_url: str) -> str:\n    return Path(repo_url.rstrip(\"/\").split(\"/\")[-1]).stem\nasync def execute_fetch_repository(session, ctx, repo_url):\n    print(f\"repo_url: {repo_url}\")\n    repository_name = extract_alias_from_url(repo_url)\n    repo_path = WORKSPACE / repository_name    \n    if repo_path.exists():\n        await ctx.info(f\"Repository {repository_name} already exists locally\")\n    else:\n        await ctx.info(f\"Cloning repository {repository_name}...\")",
        "detail": "tools.fetch_repository",
        "documentation": {}
    },
    {
        "label": "WORKSPACE",
        "kind": 5,
        "importPath": "tools.fetch_repository",
        "description": "tools.fetch_repository",
        "peekOfCode": "WORKSPACE = Path(\"./workspace\")\nWORKSPACE.mkdir(exist_ok=True)\ndef extract_alias_from_url(repo_url: str) -> str:\n    return Path(repo_url.rstrip(\"/\").split(\"/\")[-1]).stem\nasync def execute_fetch_repository(session, ctx, repo_url):\n    print(f\"repo_url: {repo_url}\")\n    repository_name = extract_alias_from_url(repo_url)\n    repo_path = WORKSPACE / repository_name    \n    if repo_path.exists():\n        await ctx.info(f\"Repository {repository_name} already exists locally\")",
        "detail": "tools.fetch_repository",
        "documentation": {}
    },
    {
        "label": "extract_alias_from_url",
        "kind": 2,
        "importPath": "tools.classify_repository",
        "description": "tools.classify_repository",
        "peekOfCode": "def extract_alias_from_url(repo_url: str) -> str:\n    return Path(repo_url.rstrip(\"/\").split(\"/\")[-1]).stem\nasync def execute_classify_repository(session, ctx, repository_name):\n    repo_path = WORKSPACE / repository_name\n    await ctx.info(f\"Starting file classification for repository {repository_name}...\")\n    for file_path in repo_path.rglob(\"*\"):\n        if not file_path.is_file() or \".git\" in file_path.parts:\n            continue\n        file_type, language, classification = classify_by_extension(file_path)\n        if file_type == \"skip\":",
        "detail": "tools.classify_repository",
        "documentation": {}
    },
    {
        "label": "classify_by_extension",
        "kind": 2,
        "importPath": "tools.classify_repository",
        "description": "tools.classify_repository",
        "peekOfCode": "def classify_by_extension(file_path):\n    ext = file_path.suffix.lower()\n    image_ext = [\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".tiff\", \".ico\", \".webp\"]\n    if ext == \".md\":\n        return \"skip\", \"Text\", \"Markdown\"\n    if ext == \".ans\":\n        return \"skip\", ext, \"ANSI\"\n    if ext in image_ext:\n        return \"skip\", ext, \"Image\"\n    return \"process\", None, None",
        "detail": "tools.classify_repository",
        "documentation": {}
    },
    {
        "label": "safe_read_file",
        "kind": 2,
        "importPath": "tools.classify_repository",
        "description": "tools.classify_repository",
        "peekOfCode": "def safe_read_file(file_path):\n    for encoding in ['utf-8', 'cp1252', 'iso-8859-1', 'cp037']:\n        try:\n            with open(file_path, \"r\", encoding=encoding, errors=\"replace\") as f:\n                return f.read(), encoding\n        except (UnicodeDecodeError, UnicodeError):\n            continue\n    return None, None\ndef extract_json_from_text(text):\n    matches = re.findall(r\"```json\\s*(\\{.*?\\})\\s*```\", text, re.DOTALL)",
        "detail": "tools.classify_repository",
        "documentation": {}
    },
    {
        "label": "extract_json_from_text",
        "kind": 2,
        "importPath": "tools.classify_repository",
        "description": "tools.classify_repository",
        "peekOfCode": "def extract_json_from_text(text):\n    matches = re.findall(r\"```json\\s*(\\{.*?\\})\\s*```\", text, re.DOTALL)\n    if not matches:\n        raise ValueError(\"No valid JSON found in LLM response\")\n    return json.loads(matches[0])\nasync def register_document(session, repository_name, file_path, language, classification):\n    upsert_document(\n        session=session,\n        repository_name=repository_name,\n        full_path=str(file_path),",
        "detail": "tools.classify_repository",
        "documentation": {}
    },
    {
        "label": "WORKSPACE",
        "kind": 5,
        "importPath": "tools.classify_repository",
        "description": "tools.classify_repository",
        "peekOfCode": "WORKSPACE = Path(\"./workspace\")\nWORKSPACE.mkdir(exist_ok=True)\ndef extract_alias_from_url(repo_url: str) -> str:\n    return Path(repo_url.rstrip(\"/\").split(\"/\")[-1]).stem\nasync def execute_classify_repository(session, ctx, repository_name):\n    repo_path = WORKSPACE / repository_name\n    await ctx.info(f\"Starting file classification for repository {repository_name}...\")\n    for file_path in repo_path.rglob(\"*\"):\n        if not file_path.is_file() or \".git\" in file_path.parts:\n            continue",
        "detail": "tools.classify_repository",
        "documentation": {}
    },
    {
        "label": "execute_expose_workspace",
        "kind": 2,
        "importPath": "tools.expose_workspace",
        "description": "tools.expose_workspace",
        "peekOfCode": "def execute_expose_workspace(ctx):\n    results = []\n    repo_path = WORKSPACE\n    for file_path in repo_path.rglob(\"*\"):\n        if file_path.is_file():\n            results.append(file_path)\n    return results",
        "detail": "tools.expose_workspace",
        "documentation": {}
    },
    {
        "label": "WORKSPACE",
        "kind": 5,
        "importPath": "tools.expose_workspace",
        "description": "tools.expose_workspace",
        "peekOfCode": "WORKSPACE = Path(\"./workspace\")\ndef execute_expose_workspace(ctx):\n    results = []\n    repo_path = WORKSPACE\n    for file_path in repo_path.rglob(\"*\"):\n        if file_path.is_file():\n            results.append(file_path)\n    return results",
        "detail": "tools.expose_workspace",
        "documentation": {}
    },
    {
        "label": "safe_extract_json",
        "kind": 2,
        "importPath": "tools.extract_document_flow",
        "description": "tools.extract_document_flow",
        "peekOfCode": "def safe_extract_json(response_text):\n    try:\n        json_match = re.search(r'(\\{.*\\})', response_text, re.DOTALL)\n        json_content = json_match.group(1) if json_match else response_text\n        return json.loads(json_content)\n    except Exception:\n        return None",
        "detail": "tools.extract_document_flow",
        "documentation": {}
    },
    {
        "label": "retreive_document_info",
        "kind": 2,
        "importPath": "tools.document",
        "description": "tools.document",
        "peekOfCode": "def retreive_document_info(session, repository, filename):\n    result = session.run(\"\"\"\n        MATCH (documentInfo:Document {filename: $filename})\n        RETURN documentInfo\n    \"\"\", filename=filename)\n    document_info = graph_to_json(result, \"documentInfo\")\n    # print(f\"document_info: {document_info}\")\n    return document_info\nasync def classify_document(session, repository, filename, ctx):\n    try:",
        "detail": "tools.document",
        "documentation": {}
    },
    {
        "label": "get_file_content_full_path",
        "kind": 2,
        "importPath": "tools.document",
        "description": "tools.document",
        "peekOfCode": "def get_file_content_full_path(full_path: str) -> str:\n    with open(full_path, \"r\") as f:\n        return f.read()\nasync def get_file_content(session, repository, filename, ctx) -> str:\n    try:\n        document_info = retreive_document_info(session=session, repository=repository, filename=filename)[0]\n    except Exception as e:\n        await ctx.error(f\"Error: Unable to retrieve document info: {str(e)}\")\n        return \"Error: Unable to retrieve document info\"\n    return get_file_content_full_path(document_info[\"full_path\"])",
        "detail": "tools.document",
        "documentation": {}
    },
    {
        "label": "upsert_repository",
        "kind": 2,
        "importPath": "graph.graph_upsert",
        "description": "graph.graph_upsert",
        "peekOfCode": "def upsert_repository(session, repository_name):\n    session.run(\"\"\"\n        MERGE (r:Repository {repository_name: $repository_name})\n    \"\"\", repository_name=repository_name)\ndef upsert_document(session, repository_name, filename, full_path, language, classification):\n    session.run(\"\"\"\n        MATCH (r:Repository {repository_name: $repository_name})\n        MERGE (d:Document {filename: $filename})\n        SET d.full_path = $full_path,\n            d.language = $language,",
        "detail": "graph.graph_upsert",
        "documentation": {}
    },
    {
        "label": "upsert_document",
        "kind": 2,
        "importPath": "graph.graph_upsert",
        "description": "graph.graph_upsert",
        "peekOfCode": "def upsert_document(session, repository_name, filename, full_path, language, classification):\n    session.run(\"\"\"\n        MATCH (r:Repository {repository_name: $repository_name})\n        MERGE (d:Document {filename: $filename})\n        SET d.full_path = $full_path,\n            d.language = $language,\n            d.classification = $classification\n        MERGE (r)-[:CONTAINS]->(d)\n    \"\"\", repository_name=repository_name, filename=filename, full_path=full_path, language=language, classification=classification)\ndef upsert_entry_point(session, document_filename, name, node_type, is_entry_point):",
        "detail": "graph.graph_upsert",
        "documentation": {}
    },
    {
        "label": "upsert_entry_point",
        "kind": 2,
        "importPath": "graph.graph_upsert",
        "description": "graph.graph_upsert",
        "peekOfCode": "def upsert_entry_point(session, document_filename, name, node_type, is_entry_point):\n    session.run(\"\"\"\n        MATCH (d:Document {filename: $document_filename})\n        MERGE (e:EntryPoint {name: $name, document: $document_filename})\n        SET e.type = $node_type, e.is_entry_point = $is_entry_point\n        MERGE (d)-[:FLOWS_TO]->(e)\n    \"\"\", document_filename=document_filename, name=name, node_type=node_type, is_entry_point=is_entry_point)\ndef upsert_internal_edge(session, from_entry, to_internal, document_filename, transfer_type):\n    session.run(\"\"\"\n        MATCH (e:EntryPoint {name: $from_entry, document: $document_filename})",
        "detail": "graph.graph_upsert",
        "documentation": {}
    },
    {
        "label": "upsert_internal_edge",
        "kind": 2,
        "importPath": "graph.graph_upsert",
        "description": "graph.graph_upsert",
        "peekOfCode": "def upsert_internal_edge(session, from_entry, to_internal, document_filename, transfer_type):\n    session.run(\"\"\"\n        MATCH (e:EntryPoint {name: $from_entry, document: $document_filename})\n        MERGE (n:InternalCall {name: $to_internal, document: $document_filename})\n        MERGE (e)-[r:OUTBOUND_EDGE]->(n)\n        SET r.transfer_type = $transfer_type\n        MERGE (n)-[:INBOUND_EDGE]->(e)\n    \"\"\", from_entry=from_entry, to_internal=to_internal, document_filename=document_filename, transfer_type=transfer_type)\ndef upsert_external_edge(session, from_entry, target_name, integration_type, document_filename, transfer_type):\n    result = session.run(\"\"\"",
        "detail": "graph.graph_upsert",
        "documentation": {}
    },
    {
        "label": "upsert_external_edge",
        "kind": 2,
        "importPath": "graph.graph_upsert",
        "description": "graph.graph_upsert",
        "peekOfCode": "def upsert_external_edge(session, from_entry, target_name, integration_type, document_filename, transfer_type):\n    result = session.run(\"\"\"\n        MATCH (d:Document {filename: $target_name})\n        RETURN d\n    \"\"\", target_name=target_name)\n    if result.single():\n        session.run(\"\"\"\n            MATCH (src_doc:Document {filename: $document_filename})\n            MATCH (target_doc:Document {filename: $target_name})\n            MERGE (src_doc)-[r:INTEGRATES_WITH]->(target_doc)",
        "detail": "graph.graph_upsert",
        "documentation": {}
    },
    {
        "label": "get_all_repositories",
        "kind": 2,
        "importPath": "graph.graph_query",
        "description": "graph.graph_query",
        "peekOfCode": "def get_all_repositories(session):\n    result = session.run(\"\"\"\n        MATCH (r:Repository)\n        RETURN r\n    \"\"\")\n    return [record[\"r\"] for record in result]\ndef get_documents_by_repository(session, repository_name):\n    result = session.run(\"\"\"\n        MATCH (r:Repository {repository_name: $repository_name})-[:CONTAINS]->(d:Document)\n        RETURN d",
        "detail": "graph.graph_query",
        "documentation": {}
    },
    {
        "label": "get_documents_by_repository",
        "kind": 2,
        "importPath": "graph.graph_query",
        "description": "graph.graph_query",
        "peekOfCode": "def get_documents_by_repository(session, repository_name):\n    result = session.run(\"\"\"\n        MATCH (r:Repository {repository_name: $repository_name})-[:CONTAINS]->(d:Document)\n        RETURN d\n    \"\"\", repository_name=repository_name)\n    return [record[\"d\"] for record in result]\ndef get_document_details(session, filename):\n    result = session.run(\"\"\"\n        MATCH (d:Document {filename: $filename})\n        RETURN d",
        "detail": "graph.graph_query",
        "documentation": {}
    },
    {
        "label": "get_document_details",
        "kind": 2,
        "importPath": "graph.graph_query",
        "description": "graph.graph_query",
        "peekOfCode": "def get_document_details(session, filename):\n    result = session.run(\"\"\"\n        MATCH (d:Document {filename: $filename})\n        RETURN d\n    \"\"\", filename=filename)\n    record = result.single()\n    return record[\"d\"] if record else None\ndef get_document_flow(session, filename):\n    \"\"\"\n    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï„Î¿ EntryPoint ÎºÎ±Î¹ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ ÏÎ¿Î­Ï‚ Ï€ÏÎ¿Ï‚ Internal ÎºÎ±Î¹ External Calls Î³Î¹Î± Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿",
        "detail": "graph.graph_query",
        "documentation": {}
    },
    {
        "label": "get_document_flow",
        "kind": 2,
        "importPath": "graph.graph_query",
        "description": "graph.graph_query",
        "peekOfCode": "def get_document_flow(session, filename):\n    \"\"\"\n    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï„Î¿ EntryPoint ÎºÎ±Î¹ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ ÏÎ¿Î­Ï‚ Ï€ÏÎ¿Ï‚ Internal ÎºÎ±Î¹ External Calls Î³Î¹Î± Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿\n    \"\"\"\n    result = session.run(\"\"\"\n        MATCH (d:Document {filename: $filename})-[:FLOWS_TO]->(e:EntryPoint)\n        OPTIONAL MATCH path = (e)-[:OUTBOUND_EDGE*]->(target)\n        RETURN d, e, path\n    \"\"\", filename=filename)\n    return result.data()",
        "detail": "graph.graph_query",
        "documentation": {}
    },
    {
        "label": "get_cross_document_integrations",
        "kind": 2,
        "importPath": "graph.graph_query",
        "description": "graph.graph_query",
        "peekOfCode": "def get_cross_document_integrations(session, filename):\n    \"\"\"\n    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï„Î¹Ï‚ INTEGRATES_WITH ÏƒÏ‡Î­ÏƒÎµÎ¹Ï‚ Ï„Î¿Ï… Î±ÏÏ‡ÎµÎ¯Î¿Ï… Ï€ÏÎ¿Ï‚ Î¬Î»Î»Î± Documents\n    \"\"\"\n    result = session.run(\"\"\"\n        MATCH (src:Document {filename: $filename})-[:INTEGRATES_WITH]->(target:Document)\n        RETURN src, target\n    \"\"\", filename=filename)\n    return result.data()",
        "detail": "graph.graph_query",
        "documentation": {}
    },
    {
        "label": "process_flow_response",
        "kind": 2,
        "importPath": "graph.graph_flow_upsert",
        "description": "graph.graph_flow_upsert",
        "peekOfCode": "def process_flow_response(session, repository_name, filename, language, classification, flow_data):\n    \"\"\"\n    Î Î±Î¯ÏÎ½ÎµÎ¹ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± flow Ï„Î¿Ï… Î±ÏÏ‡ÎµÎ¯Î¿Ï… ÎºÎ±Î¹ Ï„Î± ÎµÎ¹ÏƒÎ¬Î³ÎµÎ¹ ÏƒÏ„Î¿ GraphDB Î¼Îµ Upserts\n    \"\"\"\n    # 1. Upsert Ï„Î¿ Document\n    upsert_document(\n        session=session,\n        repository_name=repository_name,\n        filename=filename,\n        full_path=filename,  # Î•Î´ÏŽ Î²Î¬Î¶ÎµÎ¹Ï‚ Ï„Î¿ Ï€Î»Î®ÏÎµÏ‚ path Î±Î½ Ï„Î¿ Î­Ï‡ÎµÎ¹Ï‚ Î±Î»Î»Î¿Ï",
        "detail": "graph.graph_flow_upsert",
        "documentation": {}
    },
    {
        "label": "upsert_technology_system",
        "kind": 2,
        "importPath": "graph.graph_upsert_archimate",
        "description": "graph.graph_upsert_archimate",
        "peekOfCode": "def upsert_technology_system(session, repository_name):\n    session.run(\"\"\"\n        MERGE (s:TechnologySystem {name: $repository_name})\n    \"\"\", repository_name=repository_name)\ndef upsert_technology_artifact(session, repository_name, filename, full_path, language, classification):\n    session.run(\"\"\"\n        MATCH (s:TechnologySystem {name: $repository_name})\n        MERGE (a:TechnologyArtifact {filename: $filename, full_path: $full_path})\n        SET a.language = $language, a.classification = $classification\n        MERGE (s)-[:CONTAINS]->(a)",
        "detail": "graph.graph_upsert_archimate",
        "documentation": {}
    },
    {
        "label": "upsert_technology_artifact",
        "kind": 2,
        "importPath": "graph.graph_upsert_archimate",
        "description": "graph.graph_upsert_archimate",
        "peekOfCode": "def upsert_technology_artifact(session, repository_name, filename, full_path, language, classification):\n    session.run(\"\"\"\n        MATCH (s:TechnologySystem {name: $repository_name})\n        MERGE (a:TechnologyArtifact {filename: $filename, full_path: $full_path})\n        SET a.language = $language, a.classification = $classification\n        MERGE (s)-[:CONTAINS]->(a)\n    \"\"\", repository_name=repository_name, filename=filename, full_path=full_path, language=language, classification=classification)\ndef upsert_technology_function(session, filename, func_name, is_entry=False):\n    session.run(\"\"\"\n        MATCH (a:TechnologyArtifact {filename: $filename})",
        "detail": "graph.graph_upsert_archimate",
        "documentation": {}
    },
    {
        "label": "upsert_technology_function",
        "kind": 2,
        "importPath": "graph.graph_upsert_archimate",
        "description": "graph.graph_upsert_archimate",
        "peekOfCode": "def upsert_technology_function(session, filename, func_name, is_entry=False):\n    session.run(\"\"\"\n        MATCH (a:TechnologyArtifact {filename: $filename})\n        MERGE (f:TechnologyFunction {name: $func_name, filename: $filename})\n        SET f.is_entry_point = $is_entry\n        MERGE (a)-[:FLOWS_TO]->(f)\n    \"\"\", filename=filename, func_name=func_name, is_entry=is_entry)\ndef upsert_internal_flow(session, source_name, target_name, filename, flow_type=\"PERFORM\"):\n    session.run(\"\"\"\n        MATCH (src:TechnologyFunction {name: $source_name, filename: $filename})",
        "detail": "graph.graph_upsert_archimate",
        "documentation": {}
    },
    {
        "label": "upsert_internal_flow",
        "kind": 2,
        "importPath": "graph.graph_upsert_archimate",
        "description": "graph.graph_upsert_archimate",
        "peekOfCode": "def upsert_internal_flow(session, source_name, target_name, filename, flow_type=\"PERFORM\"):\n    session.run(\"\"\"\n        MATCH (src:TechnologyFunction {name: $source_name, filename: $filename})\n        MERGE (tgt:TechnologyFunction {name: $target_name, filename: $filename})\n        MERGE (src)-[:FLOWS_TO {flow_type: $flow_type, integration: 'internal'}]->(tgt)\n    \"\"\", source_name=source_name, target_name=target_name, filename=filename, flow_type=flow_type)\ndef upsert_external_interaction(session, source_name, target_name, filename, interaction_type):\n    session.run(\"\"\"\n        MATCH (src:TechnologyFunction {name: $source_name, filename: $filename})\n        MERGE (svc:ApplicationService {name: $target_name})",
        "detail": "graph.graph_upsert_archimate",
        "documentation": {}
    },
    {
        "label": "upsert_external_interaction",
        "kind": 2,
        "importPath": "graph.graph_upsert_archimate",
        "description": "graph.graph_upsert_archimate",
        "peekOfCode": "def upsert_external_interaction(session, source_name, target_name, filename, interaction_type):\n    session.run(\"\"\"\n        MATCH (src:TechnologyFunction {name: $source_name, filename: $filename})\n        MERGE (svc:ApplicationService {name: $target_name})\n        MERGE (src)-[:INTERACTS_WITH {interaction_type: $interaction_type}]->(svc)\n    \"\"\", source_name=source_name, target_name=target_name, filename=filename, interaction_type=interaction_type)",
        "detail": "graph.graph_upsert_archimate",
        "documentation": {}
    },
    {
        "label": "mock_response",
        "kind": 5,
        "importPath": "tests.process_flow_response",
        "description": "tests.process_flow_response",
        "peekOfCode": "mock_response = {\n    \"filename\": \"workspace\\\\DOGECICS\\\\COBOL\\\\DOGEMAIN\",\n    \"flow_graph\": [\n        {\n            \"node\": \"00000-MAIN\",\n            \"type\": \"paragraph\",\n            \"is_entry_point\": True,\n            \"edges_to\": [\n                {\"target\": \"DOGE-MAIN-SCREEN\", \"transfer_type\": \"PERFORM\", \"integration_type\": \"internal\"},\n                {\"target\": \"DOGEQUIT\", \"transfer_type\": \"CALL\", \"integration_type\": \"external\"},",
        "detail": "tests.process_flow_response",
        "documentation": {}
    },
    {
        "label": "complex_mock",
        "kind": 5,
        "importPath": "tests.process_flow_response",
        "description": "tests.process_flow_response",
        "peekOfCode": "complex_mock = {\n    \"flow\": [\n        {\n            \"step\": \"DOGE-MAIN-SCREEN\",\n            \"action\": \"Convert the date and move it to DDATE.\",\n            \"sub_steps\": [\n                {\n                    \"step\": \"CONVERT-DATE\",\n                    \"cics_command\": \"EXEC CICS FORMATTIME ABSTIME(TEMP-DATE) DATESEP('/') MMDDYYYY(DDATE)\"\n                }",
        "detail": "tests.process_flow_response",
        "documentation": {}
    },
    {
        "label": "mock_llm_response",
        "kind": 5,
        "importPath": "tests.convert_llm_steps_to_flow_test",
        "description": "tests.convert_llm_steps_to_flow_test",
        "peekOfCode": "mock_llm_response = {\n    \"flow\": [\n        {\n            \"step\": \"00000-MAIN\",\n            \"action\": \"If EIBAID is equal to DFHPF3, perform XCTL to the program 'DOGEQUIT'.\",\n            \"condition\": \"IF EIBAID EQUAL TO DFHPF3 THEN EXEC CICS XCTL PROGRAM('DOGEQUIT')\"\n        },\n        {\n            \"step\": \"00000-MAIN\",\n            \"action\": \"If WOW-MENU is true, move 'T' to DOGECOMMS-AREA and perform the action 'DOGE-MAIN-SCREEN'.\",",
        "detail": "tests.convert_llm_steps_to_flow_test",
        "documentation": {}
    },
    {
        "label": "filename",
        "kind": 5,
        "importPath": "tests.convert_llm_steps_to_flow_test",
        "description": "tests.convert_llm_steps_to_flow_test",
        "peekOfCode": "filename = \"workspace\\\\DOGECICS\\\\COBOL\\\\DOGEMAIN\"\nconverted_graph = convert_llm_steps_to_flow(filename, mock_llm_response)\n# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î¿Ï‚\nprint(json.dumps(converted_graph, indent=2))",
        "detail": "tests.convert_llm_steps_to_flow_test",
        "documentation": {}
    },
    {
        "label": "converted_graph",
        "kind": 5,
        "importPath": "tests.convert_llm_steps_to_flow_test",
        "description": "tests.convert_llm_steps_to_flow_test",
        "peekOfCode": "converted_graph = convert_llm_steps_to_flow(filename, mock_llm_response)\n# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î¿Ï‚\nprint(json.dumps(converted_graph, indent=2))",
        "detail": "tests.convert_llm_steps_to_flow_test",
        "documentation": {}
    },
    {
        "label": "converted_complex",
        "kind": 5,
        "importPath": "tests.convert_complex_llm_steps_to_flow_test",
        "description": "tests.convert_complex_llm_steps_to_flow_test",
        "peekOfCode": "converted_complex = convert_llm_steps_to_flow(\"workspace\\\\DOGECICS\\\\COBOL\\\\DOGEMAIN\", complex_mock)\nimport json\nprint(json.dumps(converted_complex, indent=2))",
        "detail": "tests.convert_complex_llm_steps_to_flow_test",
        "documentation": {}
    },
    {
        "label": "process_execution_flow",
        "kind": 2,
        "importPath": "helpers.graph_flow_processor",
        "description": "helpers.graph_flow_processor",
        "peekOfCode": "def process_execution_flow(filename, llm_data):\n    nodes = {}\n    seen_edges = set()  # Î‘Ï€Î¿Ï†Ï…Î³Î® Î´Î¹Ï€Î»ÏŒÏ„Ï…Ï€Ï‰Î½ edges\n    def generate_unique_name(base, content):\n        hash_digest = hashlib.md5(content.encode()).hexdigest()[:6]\n        return f\"{base}-{hash_digest}\"\n    def process_step(step, parent=None):\n        step_name = step[\"step\"]\n        if step_name not in nodes:\n            nodes[step_name] = {",
        "detail": "helpers.graph_flow_processor",
        "documentation": {}
    },
    {
        "label": "extract_text",
        "kind": 2,
        "importPath": "helpers.graph_flow_processor",
        "description": "helpers.graph_flow_processor",
        "peekOfCode": "def extract_text(text, prefix, suffix):\n    start = text.find(prefix) + len(prefix)\n    end = text.find(suffix, start)\n    return text[start:end] if start > len(prefix) - 1 and end > start else \"UNKNOWN-TARGET\"\ndef extract_perform_target(condition):\n    tokens = condition.split()\n    for i, token in enumerate(tokens):\n        if token == \"PERFORM\" and i + 1 < len(tokens):\n            return tokens[i + 1]\n    return None",
        "detail": "helpers.graph_flow_processor",
        "documentation": {}
    },
    {
        "label": "extract_perform_target",
        "kind": 2,
        "importPath": "helpers.graph_flow_processor",
        "description": "helpers.graph_flow_processor",
        "peekOfCode": "def extract_perform_target(condition):\n    tokens = condition.split()\n    for i, token in enumerate(tokens):\n        if token == \"PERFORM\" and i + 1 < len(tokens):\n            return tokens[i + 1]\n    return None\ndef extract_call_target(condition):\n    tokens = condition.split()\n    for i, token in enumerate(tokens):\n        if token == \"CALL\" and i + 1 < len(tokens):",
        "detail": "helpers.graph_flow_processor",
        "documentation": {}
    },
    {
        "label": "extract_call_target",
        "kind": 2,
        "importPath": "helpers.graph_flow_processor",
        "description": "helpers.graph_flow_processor",
        "peekOfCode": "def extract_call_target(condition):\n    tokens = condition.split()\n    for i, token in enumerate(tokens):\n        if token == \"CALL\" and i + 1 < len(tokens):\n            return tokens[i + 1].strip(\"'\\\"\")\n    return None\ndef edge(target, transfer_type, integration_type):\n    return {\"target\": target, \"transfer_type\": transfer_type, \"integration_type\": integration_type}",
        "detail": "helpers.graph_flow_processor",
        "documentation": {}
    },
    {
        "label": "edge",
        "kind": 2,
        "importPath": "helpers.graph_flow_processor",
        "description": "helpers.graph_flow_processor",
        "peekOfCode": "def edge(target, transfer_type, integration_type):\n    return {\"target\": target, \"transfer_type\": transfer_type, \"integration_type\": integration_type}",
        "detail": "helpers.graph_flow_processor",
        "documentation": {}
    },
    {
        "label": "safe_extract_json",
        "kind": 2,
        "importPath": "helpers.response_helper",
        "description": "helpers.response_helper",
        "peekOfCode": "def safe_extract_json(response_text):\n    try:\n        json_match = re.search(r'(\\{.*\\})', response_text, re.DOTALL)\n        json_content = json_match.group(1) if json_match else response_text\n        return json.loads(json_content)\n    except Exception:\n        return None\ndef graph_to_json(response_records, alias):\n    \"\"\"\n    ÎœÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ Neo4j Nodes ÏƒÎµ JSON serializable list.",
        "detail": "helpers.response_helper",
        "documentation": {}
    },
    {
        "label": "graph_to_json",
        "kind": 2,
        "importPath": "helpers.response_helper",
        "description": "helpers.response_helper",
        "peekOfCode": "def graph_to_json(response_records, alias):\n    \"\"\"\n    ÎœÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ Neo4j Nodes ÏƒÎµ JSON serializable list.\n    \"\"\"\n    try:\n        document_info = []\n        for record in response_records:\n            node = record.get(alias)\n            if node is None:\n                continue",
        "detail": "helpers.response_helper",
        "documentation": {}
    },
    {
        "label": "get_driver",
        "kind": 2,
        "importPath": "to_be_removed.graph_db copy",
        "description": "to_be_removed.graph_db copy",
        "peekOfCode": "def get_driver():\n    # uri = os.getenv(\"NEO4J_URI\")\n    # username = os.getenv(\"NEO4J_USERNAME\")\n    # password = os.getenv(\"NEO4J_PASSWORD\")\n    uri=\"neo4j+s://e9a178b5.databases.neo4j.io\"\n    username=\"neo4j\"\n    password=\"wVycp7TYM4N3-VBFwNkY3iUEAkF4ZmZCLw7mERqqQRQ\"\n    if not uri or not username or not password:\n        raise ValueError(\"Missing required Neo4j environment variables\")\n    return GraphDatabase.driver(uri, auth=(username, password))",
        "detail": "to_be_removed.graph_db copy",
        "documentation": {}
    },
    {
        "label": "get_session",
        "kind": 2,
        "importPath": "to_be_removed.graph_db copy",
        "description": "to_be_removed.graph_db copy",
        "peekOfCode": "def get_session(driver):\n    # database = os.getenv(\"NEO4J_DATABASE\")\n    database=\"neo4j\"\n    if not database:\n        raise ValueError(\"Missing required NEO4J_DATABASE environment variable\")\n    return driver.session(database=database)\n# ----------------- Î£Ï…Î½Î±ÏÏ„Î®ÏƒÎµÎ¹Ï‚ MCP -----------------\ndef insert_repository(session, repository_name, full_path, filename, language, classification):\n    \"\"\"Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Î® ÎµÎ½Î·Î¼ÎµÏÏŽÎ½ÎµÎ¹ Î­Î½Î± Repository node\"\"\"\n    session.run(\"\"\"",
        "detail": "to_be_removed.graph_db copy",
        "documentation": {}
    },
    {
        "label": "insert_repository",
        "kind": 2,
        "importPath": "to_be_removed.graph_db copy",
        "description": "to_be_removed.graph_db copy",
        "peekOfCode": "def insert_repository(session, repository_name, full_path, filename, language, classification):\n    \"\"\"Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Î® ÎµÎ½Î·Î¼ÎµÏÏŽÎ½ÎµÎ¹ Î­Î½Î± Repository node\"\"\"\n    session.run(\"\"\"\n        MERGE (r:Repository {repository_name: $repository_name, filename: $filename})\n        SET r.full_path = $full_path,\n            r.language = $language,\n            r.classification = $classification\n    \"\"\", repository_name=repository_name, full_path=full_path, filename=filename, language=language, classification=classification)\ndef insert_field(session, repository_name, filename, name, type_, size, sudoType):\n    \"\"\"Î£Ï…Î½Î´Î­ÎµÎ¹ Field node Î¼Îµ Repository\"\"\"",
        "detail": "to_be_removed.graph_db copy",
        "documentation": {}
    },
    {
        "label": "insert_field",
        "kind": 2,
        "importPath": "to_be_removed.graph_db copy",
        "description": "to_be_removed.graph_db copy",
        "peekOfCode": "def insert_field(session, repository_name, filename, name, type_, size, sudoType):\n    \"\"\"Î£Ï…Î½Î´Î­ÎµÎ¹ Field node Î¼Îµ Repository\"\"\"\n    session.run(\"\"\"\n        MATCH (r:Repository {repository_name: $repository_name, filename: $filename})\n        CREATE (f:Field {name: $name, type: $type, size: $size, sudoType: $sudoType})\n        CREATE (r)-[:HAS_FIELD]->(f)\n    \"\"\", repository_name=repository_name, filename=filename, name=name, type=type_, size=size, sudoType=sudoType)\ndef get_all_repositories(session):\n    \"\"\"Î›Î¯ÏƒÏ„Î± ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Repositories\"\"\"\n    result = session.run(\"MATCH (r:Repository) RETURN r\")",
        "detail": "to_be_removed.graph_db copy",
        "documentation": {}
    },
    {
        "label": "get_all_repositories",
        "kind": 2,
        "importPath": "to_be_removed.graph_db copy",
        "description": "to_be_removed.graph_db copy",
        "peekOfCode": "def get_all_repositories(session):\n    \"\"\"Î›Î¯ÏƒÏ„Î± ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Repositories\"\"\"\n    result = session.run(\"MATCH (r:Repository) RETURN r\")\n    return [record[\"r\"] for record in result]\ndef get_repository(session, repository_name):\n    result = session.run(\n        \"MATCH (r:Repository {repository_name: $repository_name}) RETURN r\",\n        repository_name=repository_name\n    )\n    repositories = []",
        "detail": "to_be_removed.graph_db copy",
        "documentation": {}
    },
    {
        "label": "get_repository",
        "kind": 2,
        "importPath": "to_be_removed.graph_db copy",
        "description": "to_be_removed.graph_db copy",
        "peekOfCode": "def get_repository(session, repository_name):\n    result = session.run(\n        \"MATCH (r:Repository {repository_name: $repository_name}) RETURN r\",\n        repository_name=repository_name\n    )\n    repositories = []\n    for record in result:\n        node = record[\"r\"]\n        flat_data = {\n            \"id\": node.id,",
        "detail": "to_be_removed.graph_db copy",
        "documentation": {}
    },
    {
        "label": "get_repository_by_filename",
        "kind": 2,
        "importPath": "to_be_removed.graph_db copy",
        "description": "to_be_removed.graph_db copy",
        "peekOfCode": "def get_repository_by_filename(session, filename):\n    \"\"\"Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î¼Îµ Î²Î¬ÏƒÎ· filename\"\"\"\n    result = session.run(\"MATCH (r:Repository {filename: $filename}) RETURN r\", filename=filename)\n    return [record[\"r\"] for record in result]\ndef get_repository_by_classification(session, classification, repository_name):\n    \"\"\"Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î¼Îµ Î²Î¬ÏƒÎ· classification ÎºÎ±Î¹ repository\"\"\"\n    result = session.run(\"\"\"\n        MATCH (r:Repository {repository_name: $repository_name, classification: $classification})\n        RETURN r\n    \"\"\", repository_name=repository_name, classification=classification)",
        "detail": "to_be_removed.graph_db copy",
        "documentation": {}
    },
    {
        "label": "get_repository_by_classification",
        "kind": 2,
        "importPath": "to_be_removed.graph_db copy",
        "description": "to_be_removed.graph_db copy",
        "peekOfCode": "def get_repository_by_classification(session, classification, repository_name):\n    \"\"\"Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î¼Îµ Î²Î¬ÏƒÎ· classification ÎºÎ±Î¹ repository\"\"\"\n    result = session.run(\"\"\"\n        MATCH (r:Repository {repository_name: $repository_name, classification: $classification})\n        RETURN r\n    \"\"\", repository_name=repository_name, classification=classification)\n    return [record[\"r\"] for record in result]\ndef get_file_full_path(session, repository_name, filename):\n    \"\"\"Î›Î®ÏˆÎ· Ï„Î¿Ï… full_path ÎµÎ½ÏŒÏ‚ Î±ÏÏ‡ÎµÎ¯Î¿Ï…\"\"\"\n    result = session.run(\"\"\"",
        "detail": "to_be_removed.graph_db copy",
        "documentation": {}
    },
    {
        "label": "get_file_full_path",
        "kind": 2,
        "importPath": "to_be_removed.graph_db copy",
        "description": "to_be_removed.graph_db copy",
        "peekOfCode": "def get_file_full_path(session, repository_name, filename):\n    \"\"\"Î›Î®ÏˆÎ· Ï„Î¿Ï… full_path ÎµÎ½ÏŒÏ‚ Î±ÏÏ‡ÎµÎ¯Î¿Ï…\"\"\"\n    result = session.run(\"\"\"\n        MATCH (r:Repository {repository_name: $repository_name, filename: $filename})\n        RETURN r.full_path AS path\n    \"\"\", repository_name=repository_name, filename=filename)\n    record = result.single()\n    return record[\"path\"] if record else None\ndef close_driver(session):\n    \"\"\"ÎšÎ»ÎµÎ¯ÏƒÎ¹Î¼Î¿ Ï„Î·Ï‚ ÏƒÏÎ½Î´ÎµÏƒÎ·Ï‚\"\"\"",
        "detail": "to_be_removed.graph_db copy",
        "documentation": {}
    },
    {
        "label": "close_driver",
        "kind": 2,
        "importPath": "to_be_removed.graph_db copy",
        "description": "to_be_removed.graph_db copy",
        "peekOfCode": "def close_driver(session):\n    \"\"\"ÎšÎ»ÎµÎ¯ÏƒÎ¹Î¼Î¿ Ï„Î·Ï‚ ÏƒÏÎ½Î´ÎµÏƒÎ·Ï‚\"\"\"\n    session.driver.close()",
        "detail": "to_be_removed.graph_db copy",
        "documentation": {}
    },
    {
        "label": "query_mcp_http",
        "kind": 2,
        "importPath": "to_be_removed.local_llm",
        "description": "to_be_removed.local_llm",
        "peekOfCode": "def query_mcp_http(prompt: str) -> str:\n    \"\"\"\n    Î£Ï„Î­Î»Î½ÎµÎ¹ Ï„Î¿ prompt ÏƒÏ„Î¿Î½ MCP server Î¼Î­ÏƒÏ‰ HTTP ÎºÎ±Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï„Î·Î½ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·.\n    ÎŸ MCP server Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Ï„ÏÎ­Ï‡ÎµÎ¹ Î¼Îµ transport 'http' (Ï€.Ï‡. --transport http --port 8000).\n    \"\"\"\n    mcp_url = os.getenv(\"MCP_SERVER_URL\", \"http://localhost:8000/request\")\n    try:\n        payload = {\"prompt\": prompt}\n        headers = {\"Content-Type\": \"application/json\"}\n        resp = requests.post(mcp_url, json=payload, headers=headers, timeout=30)",
        "detail": "to_be_removed.local_llm",
        "documentation": {}
    },
    {
        "label": "query_mcp_stdio",
        "kind": 2,
        "importPath": "to_be_removed.local_llm",
        "description": "to_be_removed.local_llm",
        "peekOfCode": "def query_mcp_stdio(prompt: str) -> str:\n    \"\"\"\n    Î£Ï„Î­Î»Î½ÎµÎ¹ Ï„Î¿ prompt ÏƒÏ„Î¿Î½ MCP server Î¼Î­ÏƒÏ‰ STDIO.\n    Î•ÎºÎºÎ¹Î½Î®ÏƒÏ„Îµ Ï„Î¿Î½ server Î¼Îµ transport 'stdio'.\n    Î§ÏÎ®ÏƒÎ·:\n      export MCP_SERVER_CMD=\"python your_server.py\"\n      export MCP_USE_STDIO=true\n    \"\"\"\n    cmd = os.getenv(\"MCP_SERVER_CMD\", \"python main.py\")\n    try:",
        "detail": "to_be_removed.local_llm",
        "documentation": {}
    },
    {
        "label": "query_ollama",
        "kind": 2,
        "importPath": "to_be_removed.local_llm",
        "description": "to_be_removed.local_llm",
        "peekOfCode": "def query_ollama(prompt: str) -> str:\n    \"\"\"\n    Î£Ï„Î­Î»Î½ÎµÎ¹ Ï„Î¿ prompt ÏƒÏ„Î¿Î½ Ollama HTTP API ÎºÎ±Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï„Î·Î½ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·.\n    Î’ÎµÎ²Î±Î¹Ï‰Î¸ÎµÎ¯Ï„Îµ ÏŒÏ„Î¹ Î­Ï‡ÎµÏ„Îµ Ï„ÏÎ­Î¾ÎµÎ¹ `ollama serve`.\n    \"\"\"\n    ollama_url = os.getenv(\"OLLAMA_URL\", \"http://localhost:11434/v1/chat/completions\")\n    model = os.getenv(\"OLLAMA_MODEL\", \"deepseek-coder\")\n    headers = {\"Content-Type\": \"application/json\"}\n    payload = {\n        \"model\": model,",
        "detail": "to_be_removed.local_llm",
        "documentation": {}
    },
    {
        "label": "check_servers",
        "kind": 2,
        "importPath": "to_be_removed.local_llm",
        "description": "to_be_removed.local_llm",
        "peekOfCode": "def check_servers():\n    \"\"\"Î•Î»Î­Î³Ï‡ÎµÎ¹ Ï„Î·Î½ ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Ï„Ï‰Î½ servers\"\"\"\n    print(\"ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ·Ï‚ servers...\")\n    # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ MCP server\n    use_stdio = os.getenv(\"MCP_USE_STDIO\", \"false\").lower() == \"true\"\n    if use_stdio:\n        print(\"âœ“ MCP: Configured Î³Î¹Î± STDIO transport\")\n        mcp_cmd = os.getenv(\"MCP_SERVER_CMD\", \"python your_server.py\")\n        print(f\"  Command: {mcp_cmd}\")\n    else:",
        "detail": "to_be_removed.local_llm",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "to_be_removed.local_llm",
        "description": "to_be_removed.local_llm",
        "peekOfCode": "def main():\n    print(\"Î ÏÏŒÎ³ÏÎ±Î¼Î¼Î± ÎµÏÏ‰Ï„Î®ÏƒÎµÏ‰Î½ ÏƒÎµ MCP server & Ollama\")\n    print(\"=\" * 50)\n    # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ servers\n    check_servers()\n    print(\"Î Î»Î·ÎºÏ„ÏÎ¿Î»Î¿Î³Î®ÏƒÏ„Îµ 'exit' Î³Î¹Î± Î­Î¾Î¿Î´Î¿, 'status' Î³Î¹Î± Î­Î»ÎµÎ³Ï‡Î¿ servers.\")\n    print(\"Î ÎµÏÎ¹Î²Î±Î»Î»Î¿Î½Ï„Î¹ÎºÎ­Ï‚ Î¼ÎµÏ„Î±Î²Î»Î·Ï„Î­Ï‚:\")\n    print(f\"  MCP_USE_STDIO: {os.getenv('MCP_USE_STDIO', 'false')}\")\n    print(f\"  MCP_SERVER_URL: {os.getenv('MCP_SERVER_URL', 'http://localhost:8000/request')}\")\n    print(f\"  MCP_SERVER_CMD: {os.getenv('MCP_SERVER_CMD', 'python your_server.py')}\")",
        "detail": "to_be_removed.local_llm",
        "documentation": {}
    },
    {
        "label": "connect_to_database",
        "kind": 2,
        "importPath": "to_be_removed.database",
        "description": "to_be_removed.database",
        "peekOfCode": "def connect_to_database() -> sqlite3.Connection:\n    return sqlite3.connect(DATABASE_PATH)\ndef init_database():\n    with connect_to_database() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS repository (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                repository_name TEXT,\n                full_path TEXT,",
        "detail": "to_be_removed.database",
        "documentation": {}
    },
    {
        "label": "init_database",
        "kind": 2,
        "importPath": "to_be_removed.database",
        "description": "to_be_removed.database",
        "peekOfCode": "def init_database():\n    with connect_to_database() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS repository (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                repository_name TEXT,\n                full_path TEXT,\n                filename TEXT,\n                classification TEXT,",
        "detail": "to_be_removed.database",
        "documentation": {}
    },
    {
        "label": "insert_repository",
        "kind": 2,
        "importPath": "to_be_removed.database",
        "description": "to_be_removed.database",
        "peekOfCode": "def insert_repository(repository_name: str, full_path: str, filename: str, language: str, classification: str):\n    try:\n        with connect_to_database() as conn:\n            conn.execute(\"\"\"\n                INSERT INTO repository (repository_name, full_path, filename, language, classification)\n                VALUES (?, ?, ?, ?, ?)\n            \"\"\", (repository_name, full_path, filename, language, classification))\n    except sqlite3.Error as e:\n        print(f\"Database error: {e}\")\ndef get_repository(repository_name: str):",
        "detail": "to_be_removed.database",
        "documentation": {}
    },
    {
        "label": "get_repository",
        "kind": 2,
        "importPath": "to_be_removed.database",
        "description": "to_be_removed.database",
        "peekOfCode": "def get_repository(repository_name: str):\n    with connect_to_database() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT * FROM repository WHERE repository_name = ?\", (repository_name,))\n        return cursor.fetchall()\ndef get_all_repositories():\n    with connect_to_database() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT * FROM repository\")\n        return cursor.fetchall()",
        "detail": "to_be_removed.database",
        "documentation": {}
    },
    {
        "label": "get_all_repositories",
        "kind": 2,
        "importPath": "to_be_removed.database",
        "description": "to_be_removed.database",
        "peekOfCode": "def get_all_repositories():\n    with connect_to_database() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT * FROM repository\")\n        return cursor.fetchall()\ndef get_repository_by_filename(filename: str):\n    with connect_to_database() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT * FROM repository WHERE filename = ?\", (filename,))\n        return cursor.fetchall()",
        "detail": "to_be_removed.database",
        "documentation": {}
    },
    {
        "label": "get_repository_by_filename",
        "kind": 2,
        "importPath": "to_be_removed.database",
        "description": "to_be_removed.database",
        "peekOfCode": "def get_repository_by_filename(filename: str):\n    with connect_to_database() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT * FROM repository WHERE filename = ?\", (filename,))\n        return cursor.fetchall()\ndef get_repository_by_classification(classification: str, repository_name: str):\n    with connect_to_database() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT * FROM repository WHERE classification = ? AND repository_name = ?\", (classification, repository_name))\n        return cursor.fetchall()",
        "detail": "to_be_removed.database",
        "documentation": {}
    },
    {
        "label": "get_repository_by_classification",
        "kind": 2,
        "importPath": "to_be_removed.database",
        "description": "to_be_removed.database",
        "peekOfCode": "def get_repository_by_classification(classification: str, repository_name: str):\n    with connect_to_database() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT * FROM repository WHERE classification = ? AND repository_name = ?\", (classification, repository_name))\n        return cursor.fetchall()\ndef get_file_full_path(repository_name: str, filename: str):\n    with connect_to_database() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT full_path FROM repository WHERE repository_name = ? AND filename = ?\", (repository_name, filename))\n        return cursor.fetchone()",
        "detail": "to_be_removed.database",
        "documentation": {}
    },
    {
        "label": "get_file_full_path",
        "kind": 2,
        "importPath": "to_be_removed.database",
        "description": "to_be_removed.database",
        "peekOfCode": "def get_file_full_path(repository_name: str, filename: str):\n    with connect_to_database() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT full_path FROM repository WHERE repository_name = ? AND filename = ?\", (repository_name, filename))\n        return cursor.fetchone()",
        "detail": "to_be_removed.database",
        "documentation": {}
    },
    {
        "label": "DATABASE_PATH",
        "kind": 5,
        "importPath": "to_be_removed.database",
        "description": "to_be_removed.database",
        "peekOfCode": "DATABASE_PATH = \"mcp_analysis_data.db\"\ndef connect_to_database() -> sqlite3.Connection:\n    return sqlite3.connect(DATABASE_PATH)\ndef init_database():\n    with connect_to_database() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS repository (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                repository_name TEXT,",
        "detail": "to_be_removed.database",
        "documentation": {}
    },
    {
        "label": "get_file_content_full_path",
        "kind": 2,
        "importPath": "analysis",
        "description": "analysis",
        "peekOfCode": "def get_file_content_full_path(full_path: str) -> str:\n    with open(full_path, \"r\") as f:\n        return f.read()\ndef get_file_content(repository_name: str, filename: str) -> str:\n    \"\"\"\n    Helper function to retrieve file content from the repository.\n    This is used internally by the retrieve_file_content tool.\n    \"\"\"\n    try:\n        repo_alias = repository_name.replace(\"resource://\", \"\")",
        "detail": "analysis",
        "documentation": {}
    },
    {
        "label": "get_file_content",
        "kind": 2,
        "importPath": "analysis",
        "description": "analysis",
        "peekOfCode": "def get_file_content(repository_name: str, filename: str) -> str:\n    \"\"\"\n    Helper function to retrieve file content from the repository.\n    This is used internally by the retrieve_file_content tool.\n    \"\"\"\n    try:\n        repo_alias = repository_name.replace(\"resource://\", \"\")\n        repo_path = WORKSPACE / repo_alias\n        # Handle both absolute and relative paths\n        if os.path.isabs(filename):",
        "detail": "analysis",
        "documentation": {}
    },
    {
        "label": "WORKSPACE",
        "kind": 5,
        "importPath": "analysis",
        "description": "analysis",
        "peekOfCode": "WORKSPACE = Path(\"./workspace\")\nWORKSPACE.mkdir(exist_ok=True)\ndef get_file_content_full_path(full_path: str) -> str:\n    with open(full_path, \"r\") as f:\n        return f.read()\ndef get_file_content(repository_name: str, filename: str) -> str:\n    \"\"\"\n    Helper function to retrieve file content from the repository.\n    This is used internally by the retrieve_file_content tool.\n    \"\"\"",
        "detail": "analysis",
        "documentation": {}
    },
    {
        "label": "extract_alias_from_url",
        "kind": 2,
        "importPath": "legacy_analysis_server",
        "description": "legacy_analysis_server",
        "peekOfCode": "def extract_alias_from_url(repo_url: str) -> str:\n    return Path(repo_url.rstrip(\"/\").split(\"/\")[-1]).stem\n@mcp.tool(name=\"fetch_repository\", description=\"Clones a COBOL repository and registers it as a resource.\")\nasync def fetch_repository(repo_url: str, ctx: Context) -> str:\n    return await execute_fetch_repository(session=session, ctx=ctx, repo_url=repo_url)  \n@mcp.tool(name=\"classify_repository\", description=\"Classifies files in a repository by programming language or file type.\")\nasync def classify_repository(repository_name: str, ctx: Context) -> str:\n    return await execute_classify_repository(session=session, ctx=ctx, repository_name=repository_name)\n@mcp.tool(name=\"processed_repository\", description=\"Returns a list of file from a processed repository by name.\")\nasync def processed_repository(repository: str, ctx: Context) -> list[dict]:",
        "detail": "legacy_analysis_server",
        "documentation": {}
    },
    {
        "label": "find_copy_definition",
        "kind": 2,
        "importPath": "legacy_analysis_server",
        "description": "legacy_analysis_server",
        "peekOfCode": "def find_copy_definition(ctx: Context, resource_uri: str, copy_name: str) -> dict:\n    import re\n    # ctx.log.info(f\"Searching for COPY definition: {copy_name} in {resource_uri}\")\n    repo_alias = resource_uri.replace(\"resource://\", \"\")\n    base_path = WORKSPACE / repo_alias\n    matched_files = []\n    # Traverse all files like list_cobol_files does\n    for file_path in base_path.rglob(\"*\"):\n        # ctx.log.info(f\"Checking file: {file_path}\")\n        if not file_path.is_file():",
        "detail": "legacy_analysis_server",
        "documentation": {}
    },
    {
        "label": "return_workspace",
        "kind": 2,
        "importPath": "legacy_analysis_server",
        "description": "legacy_analysis_server",
        "peekOfCode": "def return_workspace(ctx: Context) -> list[str]:\n    results = execute_expose_workspace(ctx)\n    if results is None:\n        return []\n    return results\n@mcp.tool(\"file_classification\", description=\"Classifies file content by programming language or file type.\")\nasync def file_classification(repository_name: str, filename: str, ctx: Context) -> str:\n    \"\"\"\n    Classifies a file based on its content and filename.\n    Takes filename and content as input to avoid file system operations.",
        "detail": "legacy_analysis_server",
        "documentation": {}
    },
    {
        "label": "mcp",
        "kind": 5,
        "importPath": "legacy_analysis_server",
        "description": "legacy_analysis_server",
        "peekOfCode": "mcp = FastMCP(name=\"legacy-analyzer-v1\")\ndriver = get_driver()\nsession = get_session(driver)\nWORKSPACE = Path(\"./workspace\")\nWORKSPACE.mkdir(exist_ok=True)\n_repo_registry = {}  # Keeps track of aliases and paths\ndef extract_alias_from_url(repo_url: str) -> str:\n    return Path(repo_url.rstrip(\"/\").split(\"/\")[-1]).stem\n@mcp.tool(name=\"fetch_repository\", description=\"Clones a COBOL repository and registers it as a resource.\")\nasync def fetch_repository(repo_url: str, ctx: Context) -> str:",
        "detail": "legacy_analysis_server",
        "documentation": {}
    },
    {
        "label": "driver",
        "kind": 5,
        "importPath": "legacy_analysis_server",
        "description": "legacy_analysis_server",
        "peekOfCode": "driver = get_driver()\nsession = get_session(driver)\nWORKSPACE = Path(\"./workspace\")\nWORKSPACE.mkdir(exist_ok=True)\n_repo_registry = {}  # Keeps track of aliases and paths\ndef extract_alias_from_url(repo_url: str) -> str:\n    return Path(repo_url.rstrip(\"/\").split(\"/\")[-1]).stem\n@mcp.tool(name=\"fetch_repository\", description=\"Clones a COBOL repository and registers it as a resource.\")\nasync def fetch_repository(repo_url: str, ctx: Context) -> str:\n    return await execute_fetch_repository(session=session, ctx=ctx, repo_url=repo_url)  ",
        "detail": "legacy_analysis_server",
        "documentation": {}
    },
    {
        "label": "session",
        "kind": 5,
        "importPath": "legacy_analysis_server",
        "description": "legacy_analysis_server",
        "peekOfCode": "session = get_session(driver)\nWORKSPACE = Path(\"./workspace\")\nWORKSPACE.mkdir(exist_ok=True)\n_repo_registry = {}  # Keeps track of aliases and paths\ndef extract_alias_from_url(repo_url: str) -> str:\n    return Path(repo_url.rstrip(\"/\").split(\"/\")[-1]).stem\n@mcp.tool(name=\"fetch_repository\", description=\"Clones a COBOL repository and registers it as a resource.\")\nasync def fetch_repository(repo_url: str, ctx: Context) -> str:\n    return await execute_fetch_repository(session=session, ctx=ctx, repo_url=repo_url)  \n@mcp.tool(name=\"classify_repository\", description=\"Classifies files in a repository by programming language or file type.\")",
        "detail": "legacy_analysis_server",
        "documentation": {}
    },
    {
        "label": "WORKSPACE",
        "kind": 5,
        "importPath": "legacy_analysis_server",
        "description": "legacy_analysis_server",
        "peekOfCode": "WORKSPACE = Path(\"./workspace\")\nWORKSPACE.mkdir(exist_ok=True)\n_repo_registry = {}  # Keeps track of aliases and paths\ndef extract_alias_from_url(repo_url: str) -> str:\n    return Path(repo_url.rstrip(\"/\").split(\"/\")[-1]).stem\n@mcp.tool(name=\"fetch_repository\", description=\"Clones a COBOL repository and registers it as a resource.\")\nasync def fetch_repository(repo_url: str, ctx: Context) -> str:\n    return await execute_fetch_repository(session=session, ctx=ctx, repo_url=repo_url)  \n@mcp.tool(name=\"classify_repository\", description=\"Classifies files in a repository by programming language or file type.\")\nasync def classify_repository(repository_name: str, ctx: Context) -> str:",
        "detail": "legacy_analysis_server",
        "documentation": {}
    },
    {
        "label": "_repo_registry",
        "kind": 5,
        "importPath": "legacy_analysis_server",
        "description": "legacy_analysis_server",
        "peekOfCode": "_repo_registry = {}  # Keeps track of aliases and paths\ndef extract_alias_from_url(repo_url: str) -> str:\n    return Path(repo_url.rstrip(\"/\").split(\"/\")[-1]).stem\n@mcp.tool(name=\"fetch_repository\", description=\"Clones a COBOL repository and registers it as a resource.\")\nasync def fetch_repository(repo_url: str, ctx: Context) -> str:\n    return await execute_fetch_repository(session=session, ctx=ctx, repo_url=repo_url)  \n@mcp.tool(name=\"classify_repository\", description=\"Classifies files in a repository by programming language or file type.\")\nasync def classify_repository(repository_name: str, ctx: Context) -> str:\n    return await execute_classify_repository(session=session, ctx=ctx, repository_name=repository_name)\n@mcp.tool(name=\"processed_repository\", description=\"Returns a list of file from a processed repository by name.\")",
        "detail": "legacy_analysis_server",
        "documentation": {}
    },
    {
        "label": "server",
        "kind": 5,
        "importPath": "mcp_client",
        "description": "mcp_client",
        "peekOfCode": "server = FastMCP()\nollama_client = OllamaClient(host='http://localhost:11434')\nasync def sampling_handler(\n    messages: list,\n    params,\n    context,\n    model=\"deepseek-coder:latest\",\n) -> str:\n    prompt = \"\"\n    if params.systemPrompt:",
        "detail": "mcp_client",
        "documentation": {}
    },
    {
        "label": "ollama_client",
        "kind": 5,
        "importPath": "mcp_client",
        "description": "mcp_client",
        "peekOfCode": "ollama_client = OllamaClient(host='http://localhost:11434')\nasync def sampling_handler(\n    messages: list,\n    params,\n    context,\n    model=\"deepseek-coder:latest\",\n) -> str:\n    prompt = \"\"\n    if params.systemPrompt:\n        prompt += f\"{params.systemPrompt}\\n\\n\"",
        "detail": "mcp_client",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "mcp_client",
        "description": "mcp_client",
        "peekOfCode": "client = Client(\"http://localhost:9000/sse\", sampling_handler=sampling_handler)\n# client = Client(\"main.py\")\nasync def workflow(client: Client):\n    print(\"Fetching repository\")\n    repo_url = \"https://github.com/mainframed/DOGECICS.git\"\n    # result = await client.call_tool(\"fetch_repository\", {\"repo_url\": repo_url})\n    # repository_name = result[0].text\n    repository_name = \"DOGECICS\"\n    files_in_repository = await client.call_tool(\"processed_repository\", {\"repository\": repository_name})\n    data = json.loads(files_in_repository[0].text)",
        "detail": "mcp_client",
        "documentation": {}
    },
    {
        "label": "get_driver",
        "kind": 2,
        "importPath": "graph_db",
        "description": "graph_db",
        "peekOfCode": "def get_driver():\n    load_dotenv()\n    uri = os.getenv(\"NEO4J_URI\")\n    username = os.getenv(\"NEO4J_USERNAME\")\n    password = os.getenv(\"NEO4J_PASSWORD\")\n    print(uri, username, password)\n    if not uri or not username or not password:\n        raise ValueError(\"Missing required Neo4j connection info\")\n    return GraphDatabase.driver(uri, auth=(username, password))\ndef get_session(driver):",
        "detail": "graph_db",
        "documentation": {}
    },
    {
        "label": "get_session",
        "kind": 2,
        "importPath": "graph_db",
        "description": "graph_db",
        "peekOfCode": "def get_session(driver):\n    database = os.getenv(\"NEO4J_DATABASE\")\n    if not database:\n        raise ValueError(\"Missing required NEO4J_DATABASE\")\n    return driver.session(database=database)\n# ----------------- Î’Î¿Î·Î¸Î·Ï„Î¹ÎºÎ¬ -----------------\ndef node_to_dict(node):\n    return {\n        \"id\": node.id,\n        \"labels\": list(node.labels),",
        "detail": "graph_db",
        "documentation": {}
    },
    {
        "label": "node_to_dict",
        "kind": 2,
        "importPath": "graph_db",
        "description": "graph_db",
        "peekOfCode": "def node_to_dict(node):\n    return {\n        \"id\": node.id,\n        \"labels\": list(node.labels),\n        **node._properties\n    }\ndef get_repository(session, repository_name):\n    result = session.run(\"MATCH (r:Repository {repository_name: $repository_name}) RETURN r\", repository_name=repository_name)\n    return [node_to_dict(record[\"r\"]) for record in result]",
        "detail": "graph_db",
        "documentation": {}
    },
    {
        "label": "get_repository",
        "kind": 2,
        "importPath": "graph_db",
        "description": "graph_db",
        "peekOfCode": "def get_repository(session, repository_name):\n    result = session.run(\"MATCH (r:Repository {repository_name: $repository_name}) RETURN r\", repository_name=repository_name)\n    return [node_to_dict(record[\"r\"]) for record in result]",
        "detail": "graph_db",
        "documentation": {}
    }
]